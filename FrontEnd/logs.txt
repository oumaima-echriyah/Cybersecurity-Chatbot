
==> Audit <==
|---------|-------------------------------|----------|-------------------------|---------|---------------------|---------------------|
| Command |             Args              | Profile  |          User           | Version |     Start Time      |      End Time       |
|---------|-------------------------------|----------|-------------------------|---------|---------------------|---------------------|
| image   | load angular-frontend:latest  | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 08 Dec 24 15:35 CET |                     |
| image   | load angular-frontend:latest  | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 08 Dec 24 15:35 CET |                     |
| start   |                               | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 08 Dec 24 17:04 CET |                     |
| start   |                               | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 08 Dec 24 17:27 CET | 08 Dec 24 18:14 CET |
| service | frontend-service              | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 08 Dec 24 18:19 CET |                     |
| service | frontend-service              | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 08 Dec 24 18:22 CET | 08 Dec 24 18:48 CET |
| start   |                               | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 14 Dec 24 22:02 CET | 14 Dec 24 22:03 CET |
| service | angular-frontend-service      | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 14 Dec 24 22:14 CET |                     |
| service | cybersecurity-chatbot-service | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 14 Dec 24 22:14 CET |                     |
| service | cybersecurity-chatbot-service | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 14 Dec 24 22:18 CET |                     |
| service | cybersecurity-chatbot-service | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 14 Dec 24 22:25 CET |                     |
| tunnel  |                               | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 14 Dec 24 22:28 CET | 14 Dec 24 22:28 CET |
| service | cybersecurity-chatbot-service | minikube | DESKTOP-3JP3S4U\OUASSIM | v1.34.0 | 14 Dec 24 22:32 CET |                     |
|---------|-------------------------------|----------|-------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/12/14 22:02:42
Running on machine: DESKTOP-3JP3S4U
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1214 22:02:42.457513     872 out.go:345] Setting OutFile to fd 104 ...
I1214 22:02:42.458528     872 out.go:392] TERM=,COLORTERM=, which probably does not support color
I1214 22:02:42.458528     872 out.go:358] Setting ErrFile to fd 108...
I1214 22:02:42.458528     872 out.go:392] TERM=,COLORTERM=, which probably does not support color
W1214 22:02:42.489940     872 root.go:314] Error reading config file at C:\Users\OUASSIM\.minikube\config\config.json: open C:\Users\OUASSIM\.minikube\config\config.json: The system cannot find the file specified.
I1214 22:02:42.536776     872 out.go:352] Setting JSON to false
I1214 22:02:42.549969     872 start.go:129] hostinfo: {"hostname":"DESKTOP-3JP3S4U","uptime":12676,"bootTime":1734197486,"procs":358,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.2314 Build 26100.2314","kernelVersion":"10.0.26100.2314 Build 26100.2314","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"0d265f9d-ef21-40b4-9ef5-a0a60e171344"}
W1214 22:02:42.549969     872 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1214 22:02:42.552354     872 out.go:177] * minikube v1.34.0 on Microsoft Windows 11 Home 10.0.26100.2314 Build 26100.2314
I1214 22:02:42.555423     872 notify.go:220] Checking for updates...
I1214 22:02:42.574941     872 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1214 22:02:42.576733     872 driver.go:394] Setting default libvirt URI to qemu:///system
I1214 22:02:43.202510     872 docker.go:123] docker version: linux-24.0.6:Docker Desktop 4.24.2 (124339)
I1214 22:02:43.211339     872 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1214 22:02:45.224154     872 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.0128155s)
I1214 22:02:45.226910     872 info.go:266] docker info: {ID:4d236804-9d7b-4854-a1ce-9f572f2a9d08 Containers:8 ContainersRunning:0 ContainersPaused:0 ContainersStopped:8 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:47 OomKillDisable:true NGoroutines:73 SystemTime:2024-12-14 21:02:45.109690781 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3833516032 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I1214 22:02:45.228528     872 out.go:177] * Using the docker driver based on existing profile
I1214 22:02:45.230183     872 start.go:297] selected driver: docker
I1214 22:02:45.230183     872 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\OUASSIM:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1214 22:02:45.230183     872 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1214 22:02:45.244733     872 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1214 22:02:45.822298     872 info.go:266] docker info: {ID:4d236804-9d7b-4854-a1ce-9f572f2a9d08 Containers:8 ContainersRunning:0 ContainersPaused:0 ContainersStopped:8 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:47 OomKillDisable:true NGoroutines:73 SystemTime:2024-12-14 21:02:45.76640176 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3833516032 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I1214 22:02:46.043540     872 cni.go:84] Creating CNI manager for ""
I1214 22:02:46.044175     872 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1214 22:02:46.044175     872 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\OUASSIM:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1214 22:02:46.045891     872 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I1214 22:02:46.047624     872 cache.go:121] Beginning downloading kic base image for docker with docker
I1214 22:02:46.048183     872 out.go:177] * Pulling base image v0.0.45 ...
I1214 22:02:46.049883     872 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1214 22:02:46.050410     872 image.go:79] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1214 22:02:46.051594     872 preload.go:146] Found local preload: C:\Users\OUASSIM\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1214 22:02:46.052172     872 cache.go:56] Caching tarball of preloaded images
I1214 22:02:46.052722     872 preload.go:172] Found C:\Users\OUASSIM\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1214 22:02:46.053296     872 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1214 22:02:46.053296     872 profile.go:143] Saving config to C:\Users\OUASSIM\.minikube\profiles\minikube\config.json ...
W1214 22:02:46.342691     872 image.go:95] image docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1214 22:02:46.342691     872 cache.go:149] Downloading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1214 22:02:46.347404     872 localpath.go:151] windows sanitize: C:\Users\OUASSIM\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\OUASSIM\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1214 22:02:46.347975     872 localpath.go:151] windows sanitize: C:\Users\OUASSIM\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\OUASSIM\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1214 22:02:46.348550     872 image.go:63] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1214 22:02:46.350798     872 image.go:66] Found docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1214 22:02:46.350798     872 image.go:135] docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1214 22:02:46.350798     872 cache.go:152] successfully saved docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1214 22:02:46.350798     872 cache.go:162] Loading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1214 22:02:46.350798     872 localpath.go:151] windows sanitize: C:\Users\OUASSIM\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\OUASSIM\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1214 22:02:48.298035     872 cache.go:164] successfully loaded and using docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1214 22:02:48.298612     872 cache.go:194] Successfully downloaded all kic artifacts
I1214 22:02:48.299803     872 start.go:360] acquireMachinesLock for minikube: {Name:mk9fd5081df379d21e82fd1f1d640e928de0b47c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1214 22:02:48.300368     872 start.go:364] duration metric: took 565.1µs to acquireMachinesLock for "minikube"
I1214 22:02:48.300368     872 start.go:96] Skipping create...Using existing machine configuration
I1214 22:02:48.300919     872 fix.go:54] fixHost starting: 
I1214 22:02:48.316229     872 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1214 22:02:48.755853     872 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1214 22:02:48.755853     872 fix.go:138] unexpected machine state, will restart: <nil>
I1214 22:02:48.756961     872 out.go:177] * Restarting existing docker container for "minikube" ...
I1214 22:02:48.769288     872 cli_runner.go:164] Run: docker start minikube
I1214 22:02:50.972824     872 cli_runner.go:217] Completed: docker start minikube: (2.2035356s)
I1214 22:02:50.982799     872 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1214 22:02:51.234520     872 kic.go:430] container "minikube" state is running.
I1214 22:02:51.246884     872 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1214 22:02:51.510625     872 profile.go:143] Saving config to C:\Users\OUASSIM\.minikube\profiles\minikube\config.json ...
I1214 22:02:51.514405     872 machine.go:93] provisionDockerMachine start ...
I1214 22:02:51.524506     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:02:51.821678     872 main.go:141] libmachine: Using SSH client type: native
I1214 22:02:51.870430     872 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x95c9c0] 0x95f5a0 <nil>  [] 0s} 127.0.0.1 16641 <nil> <nil>}
I1214 22:02:51.870430     872 main.go:141] libmachine: About to run SSH command:
hostname
I1214 22:02:52.019675     872 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1214 22:02:55.589145     872 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1214 22:02:55.590212     872 ubuntu.go:169] provisioning hostname "minikube"
I1214 22:02:55.600886     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:02:56.123235     872 main.go:141] libmachine: Using SSH client type: native
I1214 22:02:56.125340     872 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x95c9c0] 0x95f5a0 <nil>  [] 0s} 127.0.0.1 16641 <nil> <nil>}
I1214 22:02:56.125340     872 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1214 22:02:56.547827     872 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1214 22:02:56.558425     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:02:56.858806     872 main.go:141] libmachine: Using SSH client type: native
I1214 22:02:56.859805     872 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x95c9c0] 0x95f5a0 <nil>  [] 0s} 127.0.0.1 16641 <nil> <nil>}
I1214 22:02:56.859805     872 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1214 22:02:57.370618     872 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1214 22:02:57.370618     872 ubuntu.go:175] set auth options {CertDir:C:\Users\OUASSIM\.minikube CaCertPath:C:\Users\OUASSIM\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\OUASSIM\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\OUASSIM\.minikube\machines\server.pem ServerKeyPath:C:\Users\OUASSIM\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\OUASSIM\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\OUASSIM\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\OUASSIM\.minikube}
I1214 22:02:57.370618     872 ubuntu.go:177] setting up certificates
I1214 22:02:57.371157     872 provision.go:84] configureAuth start
I1214 22:02:57.385828     872 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1214 22:02:57.672405     872 provision.go:143] copyHostCerts
I1214 22:02:57.699580     872 exec_runner.go:144] found C:\Users\OUASSIM\.minikube/ca.pem, removing ...
I1214 22:02:57.699580     872 exec_runner.go:203] rm: C:\Users\OUASSIM\.minikube\ca.pem
I1214 22:02:57.700115     872 exec_runner.go:151] cp: C:\Users\OUASSIM\.minikube\certs\ca.pem --> C:\Users\OUASSIM\.minikube/ca.pem (1078 bytes)
I1214 22:02:57.717974     872 exec_runner.go:144] found C:\Users\OUASSIM\.minikube/cert.pem, removing ...
I1214 22:02:57.717974     872 exec_runner.go:203] rm: C:\Users\OUASSIM\.minikube\cert.pem
I1214 22:02:57.718481     872 exec_runner.go:151] cp: C:\Users\OUASSIM\.minikube\certs\cert.pem --> C:\Users\OUASSIM\.minikube/cert.pem (1123 bytes)
I1214 22:02:57.735204     872 exec_runner.go:144] found C:\Users\OUASSIM\.minikube/key.pem, removing ...
I1214 22:02:57.735204     872 exec_runner.go:203] rm: C:\Users\OUASSIM\.minikube\key.pem
I1214 22:02:57.736269     872 exec_runner.go:151] cp: C:\Users\OUASSIM\.minikube\certs\key.pem --> C:\Users\OUASSIM\.minikube/key.pem (1679 bytes)
I1214 22:02:57.736784     872 provision.go:117] generating server cert: C:\Users\OUASSIM\.minikube\machines\server.pem ca-key=C:\Users\OUASSIM\.minikube\certs\ca.pem private-key=C:\Users\OUASSIM\.minikube\certs\ca-key.pem org=OUASSIM.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1214 22:02:58.009589     872 provision.go:177] copyRemoteCerts
I1214 22:02:58.014216     872 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1214 22:02:58.021652     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:02:58.258779     872 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:16641 SSHKeyPath:C:\Users\OUASSIM\.minikube\machines\minikube\id_rsa Username:docker}
I1214 22:02:58.461766     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1214 22:02:58.571352     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\machines\server.pem --> /etc/docker/server.pem (1180 bytes)
I1214 22:02:58.631281     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1214 22:02:58.724509     872 provision.go:87] duration metric: took 1.3528212s to configureAuth
I1214 22:02:58.724509     872 ubuntu.go:193] setting minikube options for container-runtime
I1214 22:02:58.725601     872 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1214 22:02:58.736492     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:02:58.960920     872 main.go:141] libmachine: Using SSH client type: native
I1214 22:02:58.961467     872 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x95c9c0] 0x95f5a0 <nil>  [] 0s} 127.0.0.1 16641 <nil> <nil>}
I1214 22:02:58.961467     872 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1214 22:02:59.190861     872 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1214 22:02:59.190861     872 ubuntu.go:71] root file system type: overlay
I1214 22:02:59.193051     872 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1214 22:02:59.200923     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:02:59.422411     872 main.go:141] libmachine: Using SSH client type: native
I1214 22:02:59.422970     872 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x95c9c0] 0x95f5a0 <nil>  [] 0s} 127.0.0.1 16641 <nil> <nil>}
I1214 22:02:59.422970     872 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1214 22:02:59.700171     872 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1214 22:02:59.708950     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:02:59.916521     872 main.go:141] libmachine: Using SSH client type: native
I1214 22:02:59.917062     872 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x95c9c0] 0x95f5a0 <nil>  [] 0s} 127.0.0.1 16641 <nil> <nil>}
I1214 22:02:59.917062     872 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1214 22:03:00.190216     872 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1214 22:03:00.190216     872 machine.go:96] duration metric: took 8.6758115s to provisionDockerMachine
I1214 22:03:00.190752     872 start.go:293] postStartSetup for "minikube" (driver="docker")
I1214 22:03:00.190752     872 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1214 22:03:00.195511     872 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1214 22:03:00.203328     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:03:00.441291     872 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:16641 SSHKeyPath:C:\Users\OUASSIM\.minikube\machines\minikube\id_rsa Username:docker}
I1214 22:03:00.673844     872 ssh_runner.go:195] Run: cat /etc/os-release
I1214 22:03:00.689465     872 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1214 22:03:00.689465     872 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1214 22:03:00.689465     872 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1214 22:03:00.689465     872 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1214 22:03:00.689985     872 filesync.go:126] Scanning C:\Users\OUASSIM\.minikube\addons for local assets ...
I1214 22:03:00.691093     872 filesync.go:126] Scanning C:\Users\OUASSIM\.minikube\files for local assets ...
I1214 22:03:00.691093     872 start.go:296] duration metric: took 500.3409ms for postStartSetup
I1214 22:03:00.732810     872 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1214 22:03:00.739816     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:03:00.968664     872 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:16641 SSHKeyPath:C:\Users\OUASSIM\.minikube\machines\minikube\id_rsa Username:docker}
I1214 22:03:01.184482     872 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1214 22:03:01.204055     872 fix.go:56] duration metric: took 12.9031355s for fixHost
I1214 22:03:01.204055     872 start.go:83] releasing machines lock for "minikube", held for 12.9036864s
I1214 22:03:01.213718     872 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1214 22:03:01.448575     872 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1214 22:03:01.460504     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:03:01.479398     872 ssh_runner.go:195] Run: cat /version.json
I1214 22:03:01.487602     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:03:01.734622     872 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:16641 SSHKeyPath:C:\Users\OUASSIM\.minikube\machines\minikube\id_rsa Username:docker}
I1214 22:03:01.752016     872 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:16641 SSHKeyPath:C:\Users\OUASSIM\.minikube\machines\minikube\id_rsa Username:docker}
W1214 22:03:01.951075     872 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1214 22:03:02.016995     872 ssh_runner.go:195] Run: systemctl --version
I1214 22:03:02.085372     872 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1214 22:03:02.103798     872 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1214 22:03:02.126018     872 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1214 22:03:02.129366     872 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1214 22:03:02.153021     872 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1214 22:03:02.153718     872 start.go:495] detecting cgroup driver to use...
I1214 22:03:02.153718     872 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1214 22:03:02.159586     872 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1214 22:03:02.255623     872 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1214 22:03:02.312031     872 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1214 22:03:02.350123     872 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1214 22:03:02.379137     872 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1214 22:03:02.429669     872 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1214 22:03:02.476153     872 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1214 22:03:02.535032     872 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1214 22:03:02.600482     872 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1214 22:03:02.674704     872 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1214 22:03:02.740332     872 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1214 22:03:02.805455     872 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1214 22:03:02.845896     872 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1214 22:03:02.874180     872 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1214 22:03:02.897195     872 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1214 22:03:03.254912     872 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1214 22:03:03.878853     872 start.go:495] detecting cgroup driver to use...
I1214 22:03:03.878853     872 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1214 22:03:03.882122     872 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1214 22:03:03.906055     872 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1214 22:03:03.912921     872 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1214 22:03:03.943431     872 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1214 22:03:04.007712     872 ssh_runner.go:195] Run: which cri-dockerd
I1214 22:03:04.020173     872 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1214 22:03:04.044556     872 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
W1214 22:03:04.049900     872 out.go:270] ! Failing to connect to https://registry.k8s.io/ from both inside the minikube container and host machine
W1214 22:03:04.050461     872 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1214 22:03:04.088240     872 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1214 22:03:04.287164     872 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1214 22:03:04.469559     872 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1214 22:03:04.470107     872 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1214 22:03:04.525855     872 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1214 22:03:04.786274     872 ssh_runner.go:195] Run: sudo systemctl restart docker
I1214 22:03:06.196445     872 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.4101713s)
I1214 22:03:06.199761     872 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1214 22:03:06.237657     872 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1214 22:03:06.264484     872 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1214 22:03:06.285438     872 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1214 22:03:06.503639     872 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1214 22:03:06.696495     872 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1214 22:03:07.035765     872 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1214 22:03:07.078172     872 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1214 22:03:07.107671     872 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1214 22:03:07.326849     872 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1214 22:03:08.125504     872 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1214 22:03:08.160097     872 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1214 22:03:08.186327     872 start.go:563] Will wait 60s for crictl version
I1214 22:03:08.216128     872 ssh_runner.go:195] Run: which crictl
I1214 22:03:08.230028     872 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1214 22:03:08.800384     872 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1214 22:03:08.805818     872 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1214 22:03:09.160027     872 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1214 22:03:09.582032     872 out.go:235] * Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1214 22:03:09.615370     872 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1214 22:03:11.359663     872 cli_runner.go:217] Completed: docker exec -t minikube dig +short host.docker.internal: (1.7442933s)
I1214 22:03:11.360220     872 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1214 22:03:11.368994     872 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1214 22:03:11.380482     872 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1214 22:03:11.420946     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1214 22:03:11.692072     872 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\OUASSIM:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1214 22:03:11.697136     872 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1214 22:03:11.705701     872 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1214 22:03:11.885934     872 docker.go:685] Got preloaded images: -- stdout --
ouassim012/angular-frontend:latest
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1214 22:03:11.885934     872 docker.go:615] Images already preloaded, skipping extraction
I1214 22:03:11.895161     872 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1214 22:03:12.063542     872 docker.go:685] Got preloaded images: -- stdout --
ouassim012/angular-frontend:latest
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1214 22:03:12.064082     872 cache_images.go:84] Images are preloaded, skipping loading
I1214 22:03:12.064632     872 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1214 22:03:12.067312     872 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1214 22:03:12.075073     872 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1214 22:03:12.546315     872 cni.go:84] Creating CNI manager for ""
I1214 22:03:12.546315     872 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1214 22:03:12.548288     872 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1214 22:03:12.551564     872 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1214 22:03:12.557915     872 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1214 22:03:12.561132     872 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1214 22:03:12.590178     872 binaries.go:44] Found k8s binaries, skipping transfer
I1214 22:03:12.592330     872 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1214 22:03:12.611449     872 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1214 22:03:12.655810     872 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1214 22:03:12.707346     872 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1214 22:03:12.775936     872 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1214 22:03:12.785606     872 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1214 22:03:12.813345     872 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1214 22:03:13.014673     872 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1214 22:03:13.061466     872 certs.go:68] Setting up C:\Users\OUASSIM\.minikube\profiles\minikube for IP: 192.168.49.2
I1214 22:03:13.061466     872 certs.go:194] generating shared ca certs ...
I1214 22:03:13.062002     872 certs.go:226] acquiring lock for ca certs: {Name:mk4e1a9af4ac8de567a918782e884129bf911f7f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1214 22:03:13.091525     872 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\OUASSIM\.minikube\ca.key
I1214 22:03:13.143088     872 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\OUASSIM\.minikube\proxy-client-ca.key
I1214 22:03:13.149204     872 certs.go:256] generating profile certs ...
I1214 22:03:13.151785     872 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\OUASSIM\.minikube\profiles\minikube\client.key
I1214 22:03:13.190606     872 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\OUASSIM\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1214 22:03:13.260143     872 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\OUASSIM\.minikube\profiles\minikube\proxy-client.key
I1214 22:03:13.268979     872 certs.go:484] found cert: C:\Users\OUASSIM\.minikube\certs\ca-key.pem (1679 bytes)
I1214 22:03:13.269517     872 certs.go:484] found cert: C:\Users\OUASSIM\.minikube\certs\ca.pem (1078 bytes)
I1214 22:03:13.270596     872 certs.go:484] found cert: C:\Users\OUASSIM\.minikube\certs\cert.pem (1123 bytes)
I1214 22:03:13.273246     872 certs.go:484] found cert: C:\Users\OUASSIM\.minikube\certs\key.pem (1679 bytes)
I1214 22:03:13.299932     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1214 22:03:13.367422     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1214 22:03:13.441990     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1214 22:03:13.521523     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1214 22:03:13.593649     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1214 22:03:13.661438     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1214 22:03:13.731236     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1214 22:03:13.824753     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1214 22:03:13.911970     872 ssh_runner.go:362] scp C:\Users\OUASSIM\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1214 22:03:13.977087     872 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1214 22:03:14.045969     872 ssh_runner.go:195] Run: openssl version
I1214 22:03:14.068814     872 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1214 22:03:14.162554     872 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1214 22:03:14.173437     872 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Dec  8 17:13 /usr/share/ca-certificates/minikubeCA.pem
I1214 22:03:14.180993     872 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1214 22:03:14.246261     872 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1214 22:03:14.334166     872 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1214 22:03:14.402683     872 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1214 22:03:14.481057     872 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1214 22:03:14.519479     872 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1214 22:03:14.835690     872 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1214 22:03:14.876008     872 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1214 22:03:14.910437     872 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1214 22:03:14.942256     872 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\OUASSIM:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1214 22:03:14.952524     872 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1214 22:03:15.180622     872 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1214 22:03:15.216126     872 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1214 22:03:15.217199     872 kubeadm.go:593] restartPrimaryControlPlane start ...
I1214 22:03:15.222053     872 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1214 22:03:15.275086     872 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1214 22:03:15.288097     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1214 22:03:15.579577     872 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:50646"
I1214 22:03:15.581180     872 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:50646, want: 127.0.0.1:16645
I1214 22:03:15.584912     872 kubeconfig.go:62] C:\Users\OUASSIM\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I1214 22:03:15.595490     872 lock.go:35] WriteFile acquiring C:\Users\OUASSIM\.kube\config: {Name:mk1471991263f57e3fe5922a943e2eb3331041fb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1214 22:03:15.836256     872 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1214 22:03:15.871986     872 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1214 22:03:15.872530     872 kubeadm.go:597] duration metric: took 655.3315ms to restartPrimaryControlPlane
I1214 22:03:15.873094     872 kubeadm.go:394] duration metric: took 931.3664ms to StartCluster
I1214 22:03:15.873094     872 settings.go:142] acquiring lock: {Name:mk17fcd4b24c97ecda256f7dadc45b013563abb6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1214 22:03:15.873094     872 settings.go:150] Updating kubeconfig:  C:\Users\OUASSIM\.kube\config
I1214 22:03:15.873633     872 lock.go:35] WriteFile acquiring C:\Users\OUASSIM\.kube\config: {Name:mk1471991263f57e3fe5922a943e2eb3331041fb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1214 22:03:15.882277     872 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1214 22:03:15.885500     872 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1214 22:03:15.887168     872 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1214 22:03:15.887168     872 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1214 22:03:15.888417     872 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1214 22:03:15.890571     872 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1214 22:03:15.890571     872 addons.go:243] addon storage-provisioner should already be in state true
I1214 22:03:15.890571     872 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1214 22:03:15.897223     872 out.go:177] * Verifying Kubernetes components...
I1214 22:03:15.901032     872 host.go:66] Checking if "minikube" exists ...
I1214 22:03:15.920265     872 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1214 22:03:15.929047     872 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1214 22:03:15.936533     872 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1214 22:03:16.253445     872 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1214 22:03:16.253445     872 addons.go:243] addon default-storageclass should already be in state true
I1214 22:03:16.253445     872 host.go:66] Checking if "minikube" exists ...
I1214 22:03:16.255562     872 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1214 22:03:16.269076     872 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1214 22:03:16.271774     872 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1214 22:03:16.303365     872 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1214 22:03:16.303365     872 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1214 22:03:16.332910     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:03:16.338690     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1214 22:03:16.567575     872 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1214 22:03:16.567575     872 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1214 22:03:16.574528     872 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1214 22:03:16.586915     872 api_server.go:52] waiting for apiserver process to appear ...
I1214 22:03:16.597286     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 22:03:16.601586     872 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:16641 SSHKeyPath:C:\Users\OUASSIM\.minikube\machines\minikube\id_rsa Username:docker}
I1214 22:03:16.818300     872 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:16641 SSHKeyPath:C:\Users\OUASSIM\.minikube\machines\minikube\id_rsa Username:docker}
I1214 22:03:16.837870     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1214 22:03:17.065723     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1214 22:03:17.104736     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 22:03:17.861973     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:17.858717     872 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.0208466s)
W1214 22:03:17.863629     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:17.864169     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 22:03:17.873926     872 retry.go:31] will retry after 154.955154ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:17.873926     872 retry.go:31] will retry after 240.007116ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:18.038571     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1214 22:03:18.094666     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 22:03:18.121256     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1214 22:03:18.324578     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:18.324578     872 retry.go:31] will retry after 265.935259ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1214 22:03:18.403104     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:18.403104     872 retry.go:31] will retry after 503.306665ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:18.597578     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 22:03:18.598190     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1214 22:03:18.804641     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:18.804641     872 retry.go:31] will retry after 427.573529ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:18.919542     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1214 22:03:19.085778     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:19.085778     872 retry.go:31] will retry after 735.403197ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:19.105326     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 22:03:19.243252     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1214 22:03:19.408633     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:19.408633     872 retry.go:31] will retry after 1.149294932s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:19.604402     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 22:03:19.837946     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1214 22:03:20.100444     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 22:03:20.400371     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:20.400371     872 retry.go:31] will retry after 762.453992ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:20.564089     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1214 22:03:20.593727     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1214 22:03:20.657794     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:20.657794     872 retry.go:31] will retry after 1.675628438s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:21.104360     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 22:03:21.166428     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1214 22:03:21.258217     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:21.258217     872 retry.go:31] will retry after 841.932268ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:21.605488     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 22:03:22.099722     872 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1214 22:03:22.103299     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1214 22:03:22.351106     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1214 22:03:22.384453     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:22.384453     872 retry.go:31] will retry after 1.248095116s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:22.384453     872 api_server.go:72] duration metric: took 6.5021762s to wait for apiserver process to appear ...
I1214 22:03:22.384991     872 api_server.go:88] waiting for apiserver healthz status ...
I1214 22:03:22.385542     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:22.419268     872 api_server.go:269] stopped: https://127.0.0.1:16645/healthz: Get "https://127.0.0.1:16645/healthz": EOF
I1214 22:03:22.890010     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:22.895375     872 api_server.go:269] stopped: https://127.0.0.1:16645/healthz: Get "https://127.0.0.1:16645/healthz": EOF
W1214 22:03:22.977335     872 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:22.977335     872 retry.go:31] will retry after 1.907954712s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1214 22:03:23.393077     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:23.397084     872 api_server.go:269] stopped: https://127.0.0.1:16645/healthz: Get "https://127.0.0.1:16645/healthz": EOF
I1214 22:03:23.643046     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1214 22:03:23.899028     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:23.905559     872 api_server.go:269] stopped: https://127.0.0.1:16645/healthz: Get "https://127.0.0.1:16645/healthz": EOF
I1214 22:03:24.392352     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:24.403112     872 api_server.go:269] stopped: https://127.0.0.1:16645/healthz: Get "https://127.0.0.1:16645/healthz": EOF
I1214 22:03:24.898865     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:24.903594     872 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1214 22:03:29.901033     872 api_server.go:269] stopped: https://127.0.0.1:16645/healthz: Get "https://127.0.0.1:16645/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1214 22:03:29.901033     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:31.504169     872 api_server.go:279] https://127.0.0.1:16645/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1214 22:03:31.504169     872 api_server.go:103] status: https://127.0.0.1:16645/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1214 22:03:31.504169     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:31.691096     872 api_server.go:279] https://127.0.0.1:16645/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1214 22:03:31.691096     872 api_server.go:103] status: https://127.0.0.1:16645/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1214 22:03:31.898202     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:32.084798     872 api_server.go:279] https://127.0.0.1:16645/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 22:03:32.084798     872 api_server.go:103] status: https://127.0.0.1:16645/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 22:03:32.389975     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:32.416091     872 api_server.go:279] https://127.0.0.1:16645/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 22:03:32.416091     872 api_server.go:103] status: https://127.0.0.1:16645/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 22:03:32.899482     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:32.982775     872 api_server.go:279] https://127.0.0.1:16645/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 22:03:32.982775     872 api_server.go:103] status: https://127.0.0.1:16645/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 22:03:33.389425     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:33.401755     872 api_server.go:279] https://127.0.0.1:16645/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 22:03:33.401755     872 api_server.go:103] status: https://127.0.0.1:16645/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 22:03:33.896071     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:33.912547     872 api_server.go:279] https://127.0.0.1:16645/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 22:03:33.912547     872 api_server.go:103] status: https://127.0.0.1:16645/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 22:03:34.402091     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:34.426536     872 api_server.go:279] https://127.0.0.1:16645/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 22:03:34.426536     872 api_server.go:103] status: https://127.0.0.1:16645/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 22:03:34.483505     872 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (9.5799106s)
I1214 22:03:34.483505     872 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (10.8404594s)
I1214 22:03:34.882246     872 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I1214 22:03:34.883312     872 addons.go:510] duration metric: took 19.0031839s for enable addons: enabled=[storage-provisioner default-storageclass]
I1214 22:03:34.891823     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:34.989908     872 api_server.go:279] https://127.0.0.1:16645/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1214 22:03:34.989908     872 api_server.go:103] status: https://127.0.0.1:16645/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1214 22:03:35.399023     872 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:16645/healthz ...
I1214 22:03:35.479453     872 api_server.go:279] https://127.0.0.1:16645/healthz returned 200:
ok
I1214 22:03:35.491386     872 api_server.go:141] control plane version: v1.31.0
I1214 22:03:35.492500     872 api_server.go:131] duration metric: took 13.1075092s to wait for apiserver health ...
I1214 22:03:35.495050     872 system_pods.go:43] waiting for kube-system pods to appear ...
I1214 22:03:35.541774     872 system_pods.go:59] 7 kube-system pods found
I1214 22:03:35.542309     872 system_pods.go:61] "coredns-6f6b679f8f-np6qq" [2333ae1d-3b8d-4c0f-ab71-bf95e37fe59b] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1214 22:03:35.542309     872 system_pods.go:61] "etcd-minikube" [347c2702-8122-4106-9bbb-90de87012fac] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1214 22:03:35.542309     872 system_pods.go:61] "kube-apiserver-minikube" [59a4d43d-ccbc-435a-a473-0071862416dc] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1214 22:03:35.542309     872 system_pods.go:61] "kube-controller-manager-minikube" [08498f82-86f8-4d9e-bea5-4773b6dd8d9d] Running
I1214 22:03:35.542309     872 system_pods.go:61] "kube-proxy-plpjx" [10b49ad2-a944-467b-9369-7eec55f67f02] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1214 22:03:35.542309     872 system_pods.go:61] "kube-scheduler-minikube" [29985e86-bb0d-4092-94f1-647752d2e7d9] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1214 22:03:35.542309     872 system_pods.go:61] "storage-provisioner" [c3e18dbe-1601-4989-a9e8-a23bbd14052e] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1214 22:03:35.542309     872 system_pods.go:74] duration metric: took 46.7345ms to wait for pod list to return data ...
I1214 22:03:35.542842     872 kubeadm.go:582] duration metric: took 19.6605648s to wait for: map[apiserver:true system_pods:true]
I1214 22:03:35.543385     872 node_conditions.go:102] verifying NodePressure condition ...
I1214 22:03:35.599005     872 node_conditions.go:122] node storage ephemeral capacity is 263112772Ki
I1214 22:03:35.599550     872 node_conditions.go:123] node cpu capacity is 12
I1214 22:03:35.604416     872 node_conditions.go:105] duration metric: took 61.0309ms to run NodePressure ...
I1214 22:03:35.608261     872 start.go:241] waiting for startup goroutines ...
I1214 22:03:35.608261     872 start.go:246] waiting for cluster config update ...
I1214 22:03:35.609343     872 start.go:255] writing updated cluster config ...
I1214 22:03:35.764192     872 ssh_runner.go:195] Run: rm -f paused
I1214 22:03:36.399302     872 start.go:600] kubectl: 1.27.2, cluster: 1.31.0 (minor skew: 4)
I1214 22:03:36.400936     872 out.go:201] 
W1214 22:03:36.403884     872 out.go:270] ! C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.27.2, which may have incompatibilities with Kubernetes 1.31.0.
I1214 22:03:36.406547     872 out.go:177]   - Want kubectl v1.31.0? Try 'minikube kubectl -- get pods -A'
I1214 22:03:36.422202     872 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Dec 14 21:03:07 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Dec 14 21:03:07 minikube cri-dockerd[1451]: time="2024-12-14T21:03:07Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Dec 14 21:03:07 minikube cri-dockerd[1451]: time="2024-12-14T21:03:07Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Dec 14 21:03:07 minikube cri-dockerd[1451]: time="2024-12-14T21:03:07Z" level=info msg="Start docker client with request timeout 0s"
Dec 14 21:03:07 minikube cri-dockerd[1451]: time="2024-12-14T21:03:07Z" level=info msg="Hairpin mode is set to hairpin-veth"
Dec 14 21:03:08 minikube cri-dockerd[1451]: time="2024-12-14T21:03:08Z" level=info msg="Loaded network plugin cni"
Dec 14 21:03:08 minikube cri-dockerd[1451]: time="2024-12-14T21:03:08Z" level=info msg="Docker cri networking managed by network plugin cni"
Dec 14 21:03:08 minikube cri-dockerd[1451]: time="2024-12-14T21:03:08Z" level=info msg="Setting cgroupDriver cgroupfs"
Dec 14 21:03:08 minikube cri-dockerd[1451]: time="2024-12-14T21:03:08Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Dec 14 21:03:08 minikube cri-dockerd[1451]: time="2024-12-14T21:03:08Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 14 21:03:08 minikube cri-dockerd[1451]: time="2024-12-14T21:03:08Z" level=info msg="Start cri-dockerd grpc backend"
Dec 14 21:03:08 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Dec 14 21:03:14 minikube cri-dockerd[1451]: time="2024-12-14T21:03:14Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"angular-frontend-5784f46789-bn5hl_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"259f0ab47acd945005e5da07db25c1144536f74838e58844e0c91b2e2e07980c\""
Dec 14 21:03:15 minikube cri-dockerd[1451]: time="2024-12-14T21:03:15Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"angular-frontend-5784f46789-qznw4_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2cbeda1f5c108921bb79648a49efeb929c57ca429750d22d971f62fb8a44a808\""
Dec 14 21:03:15 minikube cri-dockerd[1451]: time="2024-12-14T21:03:15Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-np6qq_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c64d697002b23b39d2dafc5d67a4297916bb5e26217cd019f576cb33e709bc9e\""
Dec 14 21:03:20 minikube cri-dockerd[1451]: time="2024-12-14T21:03:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c04b5ac1df01292be14b375bc150e1cbec96b323de0a7e56dffd60b1f9ac8082/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 14 21:03:20 minikube cri-dockerd[1451]: time="2024-12-14T21:03:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/23fcfa013c0b547004e58f2888f39dc5428468d9258c8e9ded31bd6bc219129a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 14 21:03:20 minikube cri-dockerd[1451]: time="2024-12-14T21:03:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/96266e3c93b2cd4b6b23b4e2e5e5cf1c7b24cb94f5a26e13547d7ac0bbc2a9ce/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 14 21:03:20 minikube cri-dockerd[1451]: time="2024-12-14T21:03:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d508f0e48619c8b7b590feda46bb1d977fa58e446e0322d7727251708b46af94/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 14 21:03:32 minikube cri-dockerd[1451]: time="2024-12-14T21:03:32Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 14 21:03:36 minikube cri-dockerd[1451]: time="2024-12-14T21:03:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68568764a6dee9d31e488c55cc473b618bae9eee28c9738a1455399945267024/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 14 21:03:36 minikube cri-dockerd[1451]: time="2024-12-14T21:03:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f9a24cbea49c986d5a0fe065064bbeb8abcb6c5f46feeeec63a1b57db4885599/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 14 21:03:40 minikube cri-dockerd[1451]: time="2024-12-14T21:03:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d0b5f2dfa61cbee460170332020243fd530e1b2c11279e345c5e688f2d093a83/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 14 21:03:40 minikube cri-dockerd[1451]: time="2024-12-14T21:03:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2740b65415bb54fb093c9b3a0466918bed368b3e8a8d2fb6da8c69c855a2c290/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 14 21:03:40 minikube cri-dockerd[1451]: time="2024-12-14T21:03:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a4224a96bc0bba8cc8165183f1abebd560b16f2f12a2be1349148ccc642bae75/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 14 21:03:43 minikube cri-dockerd[1451]: time="2024-12-14T21:03:43Z" level=info msg="Stop pulling image ouassim012/angular-frontend:latest: Status: Image is up to date for ouassim012/angular-frontend:latest"
Dec 14 21:03:45 minikube cri-dockerd[1451]: time="2024-12-14T21:03:45Z" level=info msg="Stop pulling image ouassim012/angular-frontend:latest: Status: Image is up to date for ouassim012/angular-frontend:latest"
Dec 14 21:03:50 minikube dockerd[1098]: time="2024-12-14T21:03:50.038884955Z" level=info msg="ignoring event" container=6b704ec8c17a21b9da06667f54e0fed9f65989fbae2d3a664ffd65532175c573 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:05:31 minikube dockerd[1098]: time="2024-12-14T21:05:31.710415578Z" level=info msg="ignoring event" container=9bfe22566d3ee5b2e22ce0f47be39d90a8e63d81a78b6ffcfb055a24ecaa3491 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:11:29 minikube cri-dockerd[1451]: time="2024-12-14T21:11:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/75517d91568e3e9a4623fccade3786b64232d202afc7a8239f05ebbc57f70d71/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 14 21:11:29 minikube cri-dockerd[1451]: time="2024-12-14T21:11:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/75d56da3dd8d899980b3bb8ace7ac6aaf27d377f0825d203071ac5ad2d605c51/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 14 21:11:32 minikube cri-dockerd[1451]: time="2024-12-14T21:11:32Z" level=info msg="Stop pulling image ouassim012/cybersecurity:frontend-app: Status: Downloaded newer image for ouassim012/cybersecurity:frontend-app"
Dec 14 21:11:34 minikube cri-dockerd[1451]: time="2024-12-14T21:11:34Z" level=info msg="Stop pulling image ouassim012/cybersecurity:frontend-app: Status: Image is up to date for ouassim012/cybersecurity:frontend-app"
Dec 14 21:20:25 minikube dockerd[1098]: time="2024-12-14T21:20:25.608887045Z" level=info msg="ignoring event" container=2e3750732daf9463cc3a7b8ed6cf1f029490d35311491173b9f108595c673ad2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:20:25 minikube dockerd[1098]: time="2024-12-14T21:20:25.610685635Z" level=info msg="ignoring event" container=88d116591b2ccaff56a1a2c4d72d566286984ba9d00e90378c4ce577bd40e38c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:20:26 minikube dockerd[1098]: time="2024-12-14T21:20:26.072034721Z" level=info msg="ignoring event" container=75517d91568e3e9a4623fccade3786b64232d202afc7a8239f05ebbc57f70d71 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:20:26 minikube cri-dockerd[1451]: time="2024-12-14T21:20:26Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ouassim012-cybersecurity-57658fd7dc-kmpgw_default\": unexpected command output nsenter: cannot open /proc/5170/ns/net: No such file or directory\n with error: exit status 1"
Dec 14 21:20:26 minikube dockerd[1098]: time="2024-12-14T21:20:26.130589881Z" level=info msg="ignoring event" container=75d56da3dd8d899980b3bb8ace7ac6aaf27d377f0825d203071ac5ad2d605c51 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:21:25 minikube dockerd[1098]: time="2024-12-14T21:21:25.481401157Z" level=info msg="ignoring event" container=5ef0a37ad669e194d813c43cf2a56580bf4f6a170d99fe2d5acb1a04ebffde37 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:21:25 minikube dockerd[1098]: time="2024-12-14T21:21:25.509979790Z" level=info msg="ignoring event" container=3c95747ff5af770e8dec8433243964525ad5def6502f24fc65f8e5d3f05f3fb4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:21:26 minikube dockerd[1098]: time="2024-12-14T21:21:26.086040690Z" level=info msg="ignoring event" container=d0b5f2dfa61cbee460170332020243fd530e1b2c11279e345c5e688f2d093a83 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:21:26 minikube dockerd[1098]: time="2024-12-14T21:21:26.187769820Z" level=info msg="ignoring event" container=2740b65415bb54fb093c9b3a0466918bed368b3e8a8d2fb6da8c69c855a2c290 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:21:26 minikube cri-dockerd[1451]: time="2024-12-14T21:21:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/39c8615a32dbde01fe53ca63b4a69cb60d598aae42b72b2bb3b09e80efafac4e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 14 21:21:26 minikube cri-dockerd[1451]: time="2024-12-14T21:21:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f1a57eef89a4bfa24dddefc1b84579c5a1538ff1e06a503be04cefdc696598b8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 14 21:21:34 minikube cri-dockerd[1451]: time="2024-12-14T21:21:34Z" level=info msg="Stop pulling image ouassim012/angular-frontend:latest: Status: Image is up to date for ouassim012/angular-frontend:latest"
Dec 14 21:21:36 minikube cri-dockerd[1451]: time="2024-12-14T21:21:36Z" level=info msg="Stop pulling image ouassim012/angular-frontend:latest: Status: Image is up to date for ouassim012/angular-frontend:latest"
Dec 14 21:22:16 minikube dockerd[1098]: time="2024-12-14T21:22:16.291590274Z" level=info msg="ignoring event" container=862a259b6b7062e3598b1fa4af05ef2b4adb4905c492aa01d3c9c136bc8ef713 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:22:16 minikube dockerd[1098]: time="2024-12-14T21:22:16.375468047Z" level=info msg="ignoring event" container=89fc744cefa78791040c64cfd78b19e5c489702005ed1fc50f67123efc46e831 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:22:16 minikube dockerd[1098]: time="2024-12-14T21:22:16.819476981Z" level=info msg="ignoring event" container=39c8615a32dbde01fe53ca63b4a69cb60d598aae42b72b2bb3b09e80efafac4e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:22:17 minikube dockerd[1098]: time="2024-12-14T21:22:17.175458282Z" level=info msg="ignoring event" container=f1a57eef89a4bfa24dddefc1b84579c5a1538ff1e06a503be04cefdc696598b8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:22:17 minikube cri-dockerd[1451]: time="2024-12-14T21:22:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/53aa9ada32a3700c82de47f3bb58bee320ace9782d33cde7cb44d8c377d72171/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 14 21:22:17 minikube cri-dockerd[1451]: time="2024-12-14T21:22:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1fa8da4644b58a4132f9cec3d84cdd6cdd17dd69342d62236fa5e28a881eca82/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 14 21:22:19 minikube cri-dockerd[1451]: time="2024-12-14T21:22:19Z" level=info msg="Stop pulling image ouassim012/angular-frontend:latest: Status: Image is up to date for ouassim012/angular-frontend:latest"
Dec 14 21:22:20 minikube dockerd[1098]: time="2024-12-14T21:22:20.718514220Z" level=info msg="ignoring event" container=6f3f722368a51d4d7e3c5c181efad26ad051eaa7f82134b1daaf9b93ac5b31be module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:22:21 minikube dockerd[1098]: time="2024-12-14T21:22:21.013041147Z" level=info msg="ignoring event" container=1fa8da4644b58a4132f9cec3d84cdd6cdd17dd69342d62236fa5e28a881eca82 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:22:21 minikube cri-dockerd[1451]: time="2024-12-14T21:22:21Z" level=info msg="Stop pulling image ouassim012/angular-frontend:latest: Status: Image is up to date for ouassim012/angular-frontend:latest"
Dec 14 21:22:22 minikube dockerd[1098]: time="2024-12-14T21:22:22.715381539Z" level=info msg="ignoring event" container=17e16268b6e3bde2cafad25eeb2a338a493e1ef40898d791765ce0287480cde9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:22:23 minikube dockerd[1098]: time="2024-12-14T21:22:23.022169444Z" level=info msg="ignoring event" container=53aa9ada32a3700c82de47f3bb58bee320ace9782d33cde7cb44d8c377d72171 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 14 21:24:40 minikube cri-dockerd[1451]: time="2024-12-14T21:24:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7fede568869297367f3a96a610ac1045350ce41d3e9ac82d43b96053b90e34dc/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 14 21:24:40 minikube cri-dockerd[1451]: time="2024-12-14T21:24:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c2e580b0b28db13a9f54253e405bb03b3ff990944d2513ad72f8f141feaa2458/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
194b3ecb30580       d0c9c681d53f0       8 minutes ago       Running             cybersecurity-chatbot     0                   7fede56886929       ouassim012-cybersecurity-57658fd7dc-zcgsw
79dd44bba7e51       d0c9c681d53f0       8 minutes ago       Running             cybersecurity-chatbot     0                   c2e580b0b28db       ouassim012-cybersecurity-57658fd7dc-4k7bh
709f0e4aa89ef       6e38f40d628db       27 minutes ago      Running             storage-provisioner       6                   f9a24cbea49c9       storage-provisioner
9bfe22566d3ee       6e38f40d628db       29 minutes ago      Exited              storage-provisioner       5                   f9a24cbea49c9       storage-provisioner
c232f1f442ad1       cbb01a7bd410d       29 minutes ago      Running             coredns                   1                   a4224a96bc0bb       coredns-6f6b679f8f-np6qq
c8e43d7d0e4a2       ad83b2ca7b09e       29 minutes ago      Running             kube-proxy                1                   68568764a6dee       kube-proxy-plpjx
ea721f236a365       604f5db92eaa8       30 minutes ago      Running             kube-apiserver            1                   23fcfa013c0b5       kube-apiserver-minikube
ef566f7464319       1766f54c897f0       30 minutes ago      Running             kube-scheduler            1                   d508f0e48619c       kube-scheduler-minikube
f59804d23cc11       2e96e5913fc06       30 minutes ago      Running             etcd                      1                   96266e3c93b2c       etcd-minikube
3f94701487851       045733566833c       30 minutes ago      Running             kube-controller-manager   1                   c04b5ac1df012       kube-controller-manager-minikube
88c8d65ca5757       cbb01a7bd410d       6 days ago          Exited              coredns                   0                   c64d697002b23       coredns-6f6b679f8f-np6qq
075f30b03d141       ad83b2ca7b09e       6 days ago          Exited              kube-proxy                0                   f8511eb2e0407       kube-proxy-plpjx
efeb97df8a2db       2e96e5913fc06       6 days ago          Exited              etcd                      0                   48222f7467ef0       etcd-minikube
8ce9cc7c8fb8a       045733566833c       6 days ago          Exited              kube-controller-manager   0                   fd28e681cf8df       kube-controller-manager-minikube
fa113f593492c       1766f54c897f0       6 days ago          Exited              kube-scheduler            0                   e92300dfccac7       kube-scheduler-minikube
3573cf260c09d       604f5db92eaa8       6 days ago          Exited              kube-apiserver            0                   fb8bdf903e347       kube-apiserver-minikube


==> coredns [88c8d65ca575] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[ERROR] plugin/errors: 2 8608573794971593758.5131204329199757088. HINFO: read udp 10.244.0.3:51615->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 8608573794971593758.5131204329199757088. HINFO: read udp 10.244.0.3:44419->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 8608573794971593758.5131204329199757088. HINFO: read udp 10.244.0.3:52260->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 8608573794971593758.5131204329199757088. HINFO: read udp 10.244.0.3:47088->192.168.65.254:53: i/o timeout
[INFO] Reloading
[ERROR] plugin/errors: 2 8141088809881320618.4695266409001195304. HINFO: read udp 10.244.0.3:39221->192.168.65.254:53: i/o timeout
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
[INFO] Reloading complete
[INFO] 127.0.0.1:47212 - 13209 "HINFO IN 8141088809881320618.4695266409001195304. udp 57 false 512" - - 0 6.003924385s
[ERROR] plugin/errors: 2 8141088809881320618.4695266409001195304. HINFO: read udp 10.244.0.3:53201->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:50242 - 16251 "HINFO IN 8141088809881320618.4695266409001195304. udp 57 false 512" NXDOMAIN qr,rd,ra 57 5.578168324s
[INFO] 127.0.0.1:41071 - 3504 "HINFO IN 8141088809881320618.4695266409001195304. udp 57 false 512" NXDOMAIN qr,rd,ra 57 2.576597649s
[INFO] 127.0.0.1:36730 - 8757 "HINFO IN 8141088809881320618.4695266409001195304. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.00352994s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.098249951s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.614369569s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.492476364s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 3.722006318s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.196304671s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.22676661s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.004564844s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.014005351s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.202504641s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.414332554s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.00244727s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.397630634s


==> coredns [c232f1f442ad] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:51262 - 25808 "HINFO IN 3420449496415405045.1658333481789270755. udp 57 false 512" - - 0 6.005102161s
[ERROR] plugin/errors: 2 3420449496415405045.1658333481789270755. HINFO: read udp 10.244.0.8:52437->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:60211 - 17193 "HINFO IN 3420449496415405045.1658333481789270755. udp 57 false 512" - - 0 6.00355774s
[ERROR] plugin/errors: 2 3420449496415405045.1658333481789270755. HINFO: read udp 10.244.0.8:60232->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:38448 - 47691 "HINFO IN 3420449496415405045.1658333481789270755. udp 57 false 512" - - 0 4.002708003s
[ERROR] plugin/errors: 2 3420449496415405045.1658333481789270755. HINFO: read udp 10.244.0.8:42992->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:32936 - 62813 "HINFO IN 3420449496415405045.1658333481789270755. udp 57 false 512" - - 0 2.001823009s
[ERROR] plugin/errors: 2 3420449496415405045.1658333481789270755. HINFO: read udp 10.244.0.8:54563->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:44529 - 48533 "HINFO IN 3420449496415405045.1658333481789270755. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.261841196s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.414489851s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.796681846s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.992471903s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.407206693s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.411740949s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_12_08T18_14_22_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 08 Dec 2024 17:14:18 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 14 Dec 2024 21:33:26 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 14 Dec 2024 21:32:12 +0000   Sun, 08 Dec 2024 17:14:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 14 Dec 2024 21:32:12 +0000   Sun, 08 Dec 2024 17:14:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 14 Dec 2024 21:32:12 +0000   Sun, 08 Dec 2024 17:14:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 14 Dec 2024 21:32:12 +0000   Sun, 08 Dec 2024 17:14:23 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  263112772Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3743668Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  263112772Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3743668Ki
  pods:               110
System Info:
  Machine ID:                 71e8aafa0d0f483fab3e1645826441c5
  System UUID:                71e8aafa0d0f483fab3e1645826441c5
  Boot ID:                    c1415762-36ca-4126-bc55-5ce4a76422a5
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     ouassim012-cybersecurity-57658fd7dc-4k7bh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m50s
  default                     ouassim012-cybersecurity-57658fd7dc-zcgsw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m50s
  kube-system                 coredns-6f6b679f8f-np6qq                     100m (0%)     0 (0%)      70Mi (1%)        170Mi (4%)     6d4h
  kube-system                 etcd-minikube                                100m (0%)     0 (0%)      100Mi (2%)       0 (0%)         6d4h
  kube-system                 kube-apiserver-minikube                      250m (2%)     0 (0%)      0 (0%)           0 (0%)         6d4h
  kube-system                 kube-controller-manager-minikube             200m (1%)     0 (0%)      0 (0%)           0 (0%)         6d4h
  kube-system                 kube-proxy-plpjx                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d4h
  kube-system                 kube-scheduler-minikube                      100m (0%)     0 (0%)      0 (0%)           0 (0%)         6d4h
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d4h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           29m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  30m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           30m                kubelet          Starting kubelet.
  Warning  CgroupV1                           30m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory            30m (x7 over 30m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              30m (x7 over 30m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               30m (x7 over 30m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            30m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     29m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Dec14 20:23]   #2  #3  #4  #5  #6  #7  #8  #9 #10 #11
[  +0.031228] PCI: Fatal: No config space access function found
[  +0.049213] PCI: System does not support PCI
[  +0.058181] kvm: no hardware support
[Dec14 20:24] FS-Cache: Duplicate cookie detected
[  +0.000848] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.001649] FS-Cache: O-cookie d=000000009f4a3a6a{9P.session} n=00000000c78db639
[  +0.001715] FS-Cache: O-key=[10] '34323934393337393035'
[  +0.001908] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000918] FS-Cache: N-cookie d=000000009f4a3a6a{9P.session} n=00000000744d3612
[  +0.000957] FS-Cache: N-key=[10] '34323934393337393035'
[ +10.931007] WSL (2) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.002855] WSL (1) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00

[  +0.004548] WSL (1) ERROR: ConfigMountFsTab:2583: Processing fstab with mount -a failed.
[  +0.004956] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000010]  failed 2
[  +0.009865] WSL (3) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.003096] WSL (1) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00

[  +0.016324] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.423398] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.024340] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003408] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003234] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001524] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +5.202743] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#575 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.010880] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#639 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.021076] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#767 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.012299] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#831 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.904348] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.012671] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.144300] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.008317] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000781] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000664] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000864] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.105719] WSL (2) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.001918] WSL (1) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00

[  +0.001355] WSL (1) ERROR: ConfigMountFsTab:2583: Processing fstab with mount -a failed.
[  +0.002399] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.006820] WSL (3) ERROR: UtilCreateProcessAndWait:666: /bin/mount failed with 2
[  +0.001180] WSL (1) ERROR: UtilCreateProcessAndWait:688: /bin/mount failed with status 0xff00

[  +0.015475] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.199254] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.007028] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001399] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000940] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001313] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Dec14 21:03] tmpfs: Unknown parameter 'noswap'


==> etcd [efeb97df8a2d] <==
{"level":"info","ts":"2024-12-09T16:17:10.733276Z","caller":"traceutil/trace.go:171","msg":"trace[1084459434] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:16632; }","duration":"2.630936643s","start":"2024-12-09T16:17:08.102300Z","end":"2024-12-09T16:17:10.733237Z","steps":["trace[1084459434] 'agreement among raft nodes before linearized reading'  (duration: 2.62938851s)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:10.733455Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:08.102226Z","time spent":"2.631208009s","remote":"127.0.0.1:48072","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":8,"response size":32,"request content":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true "}
{"level":"warn","ts":"2024-12-09T16:17:10.733986Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.311739925s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-09T16:17:10.734109Z","caller":"traceutil/trace.go:171","msg":"trace[731777163] range","detail":"{range_begin:/registry/controllers/; range_end:/registry/controllers0; response_count:0; response_revision:16632; }","duration":"2.311833383s","start":"2024-12-09T16:17:08.422204Z","end":"2024-12-09T16:17:10.734037Z","steps":["trace[731777163] 'agreement among raft nodes before linearized reading'  (duration: 2.311706422s)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:10.734203Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:08.422160Z","time spent":"2.312034435s","remote":"127.0.0.1:48024","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":0,"response size":30,"request content":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true "}
{"level":"warn","ts":"2024-12-09T16:17:10.734719Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.640603383s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/\" range_end:\"/registry/serviceaccounts0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-12-09T16:17:10.734805Z","caller":"traceutil/trace.go:171","msg":"trace[2014232093] range","detail":"{range_begin:/registry/serviceaccounts/; range_end:/registry/serviceaccounts0; response_count:0; response_revision:16632; }","duration":"2.640806058s","start":"2024-12-09T16:17:08.093946Z","end":"2024-12-09T16:17:10.734752Z","steps":["trace[2014232093] 'agreement among raft nodes before linearized reading'  (duration: 2.636529153s)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:10.734900Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:08.026404Z","time spent":"2.708483572s","remote":"127.0.0.1:48022","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":41,"response size":32,"request content":"key:\"/registry/serviceaccounts/\" range_end:\"/registry/serviceaccounts0\" count_only:true "}
{"level":"warn","ts":"2024-12-09T16:17:10.736060Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.416887222s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-12-09T16:17:10.736319Z","caller":"traceutil/trace.go:171","msg":"trace[1337240903] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:16632; }","duration":"2.417191001s","start":"2024-12-09T16:17:08.319114Z","end":"2024-12-09T16:17:10.736305Z","steps":["trace[1337240903] 'agreement among raft nodes before linearized reading'  (duration: 2.414903745s)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:10.736361Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:08.319067Z","time spent":"2.417280621s","remote":"127.0.0.1:47998","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":1,"response size":32,"request content":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true "}
{"level":"warn","ts":"2024-12-09T16:17:10.739653Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.003469885s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-09T16:17:10.739727Z","caller":"traceutil/trace.go:171","msg":"trace[431994156] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:16632; }","duration":"1.003543746s","start":"2024-12-09T16:17:09.736155Z","end":"2024-12-09T16:17:10.739699Z","steps":["trace[431994156] 'agreement among raft nodes before linearized reading'  (duration: 1.003339968s)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:10.739772Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:09.735619Z","time spent":"1.004141169s","remote":"127.0.0.1:48042","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":0,"response size":30,"request content":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true "}
{"level":"warn","ts":"2024-12-09T16:17:10.739905Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.214841441s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-09T16:17:10.742790Z","caller":"traceutil/trace.go:171","msg":"trace[843592050] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:16632; }","duration":"2.215828384s","start":"2024-12-09T16:17:08.524886Z","end":"2024-12-09T16:17:10.740715Z","steps":["trace[843592050] 'agreement among raft nodes before linearized reading'  (duration: 2.214470696s)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:10.733997Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.312623773s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-12-09T16:17:10.818017Z","caller":"traceutil/trace.go:171","msg":"trace[1119231587] range","detail":"{range_begin:/registry/storageclasses/; range_end:/registry/storageclasses0; response_count:0; response_revision:16632; }","duration":"2.39668174s","start":"2024-12-09T16:17:08.421249Z","end":"2024-12-09T16:17:10.817930Z","steps":["trace[1119231587] 'agreement among raft nodes before linearized reading'  (duration: 2.312671564s)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:10.819652Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:08.421048Z","time spent":"2.397039099s","remote":"127.0.0.1:48060","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":1,"response size":32,"request content":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" count_only:true "}
{"level":"warn","ts":"2024-12-09T16:17:10.829133Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.999754242s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1111"}
{"level":"info","ts":"2024-12-09T16:17:10.830825Z","caller":"traceutil/trace.go:171","msg":"trace[1926770354] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:16632; }","duration":"1.999903425s","start":"2024-12-09T16:17:08.829298Z","end":"2024-12-09T16:17:10.829201Z","steps":["trace[1926770354] 'agreement among raft nodes before linearized reading'  (duration: 1.905335971s)","trace[1926770354] 'range keys from bolt db'  (duration: 89.608752ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-09T16:17:10.830990Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:08.829159Z","time spent":"2.00180096s","remote":"127.0.0.1:48014","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1135,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2024-12-09T16:17:11.369259Z","caller":"traceutil/trace.go:171","msg":"trace[552237378] transaction","detail":"{read_only:false; response_revision:16633; number_of_response:1; }","duration":"304.718567ms","start":"2024-12-09T16:17:11.064499Z","end":"2024-12-09T16:17:11.369218Z","steps":["trace[552237378] 'process raft request'  (duration: 303.382782ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:11.371762Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:11.063535Z","time spent":"305.794615ms","remote":"127.0.0.1:48014","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:16632 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-12-09T16:17:11.614582Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"144.412923ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033774101145449 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:3660-second id:70cc93a743ad2768>","response":"size:43"}
{"level":"warn","ts":"2024-12-09T16:17:11.614803Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:11.131084Z","time spent":"483.7096ms","remote":"127.0.0.1:47994","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-12-09T16:17:12.056336Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"233.367438ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:134"}
{"level":"info","ts":"2024-12-09T16:17:12.056609Z","caller":"traceutil/trace.go:171","msg":"trace[861732953] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:16634; }","duration":"233.642561ms","start":"2024-12-09T16:17:11.822869Z","end":"2024-12-09T16:17:12.056512Z","steps":["trace[861732953] 'range keys from in-memory index tree'  (duration: 233.239805ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:12.386792Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.009924ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033774101145456 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:16629 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128033774101145454 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2024-12-09T16:17:12.386989Z","caller":"traceutil/trace.go:171","msg":"trace[271607602] transaction","detail":"{read_only:false; response_revision:16635; number_of_response:1; }","duration":"236.935409ms","start":"2024-12-09T16:17:12.150033Z","end":"2024-12-09T16:17:12.386968Z","steps":["trace[271607602] 'process raft request'  (duration: 114.059399ms)","trace[271607602] 'compare'  (duration: 121.103926ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-09T16:17:12.803253Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.708062ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-12-09T16:17:12.803253Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.179343ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-09T16:17:12.803562Z","caller":"traceutil/trace.go:171","msg":"trace[1343406415] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:16635; }","duration":"109.858387ms","start":"2024-12-09T16:17:12.693512Z","end":"2024-12-09T16:17:12.803371Z","steps":["trace[1343406415] 'range keys from in-memory index tree'  (duration: 109.488756ms)"],"step_count":1}
{"level":"info","ts":"2024-12-09T16:17:12.804282Z","caller":"traceutil/trace.go:171","msg":"trace[72155638] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:16635; }","duration":"124.514531ms","start":"2024-12-09T16:17:12.679029Z","end":"2024-12-09T16:17:12.803544Z","steps":["trace[72155638] 'range keys from in-memory index tree'  (duration: 124.078722ms)"],"step_count":1}
{"level":"info","ts":"2024-12-09T16:17:13.753558Z","caller":"traceutil/trace.go:171","msg":"trace[1762967380] transaction","detail":"{read_only:false; response_revision:16636; number_of_response:1; }","duration":"201.917656ms","start":"2024-12-09T16:17:13.551416Z","end":"2024-12-09T16:17:13.753334Z","steps":["trace[1762967380] 'process raft request'  (duration: 200.028566ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:16.012881Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.025231015s","expected-duration":"1s"}
{"level":"info","ts":"2024-12-09T16:17:16.013986Z","caller":"traceutil/trace.go:171","msg":"trace[1674311887] transaction","detail":"{read_only:false; response_revision:16637; number_of_response:1; }","duration":"1.026639191s","start":"2024-12-09T16:17:14.987330Z","end":"2024-12-09T16:17:16.013969Z","steps":["trace[1674311887] 'process raft request'  (duration: 1.026135332s)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:16.014473Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:14.987292Z","time spent":"1.026843448s","remote":"127.0.0.1:48036","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:16630 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2024-12-09T16:17:16.423974Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"224.734861ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033774101145472 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:16631 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:18"}
{"level":"info","ts":"2024-12-09T16:17:16.424156Z","caller":"traceutil/trace.go:171","msg":"trace[67230006] linearizableReadLoop","detail":"{readStateIndex:20816; appliedIndex:20814; }","duration":"744.446513ms","start":"2024-12-09T16:17:15.679692Z","end":"2024-12-09T16:17:16.424139Z","steps":["trace[67230006] 'read index received'  (duration: 333.727066ms)","trace[67230006] 'applied index is now lower than readState.Index'  (duration: 410.717864ms)"],"step_count":2}
{"level":"info","ts":"2024-12-09T16:17:16.424315Z","caller":"traceutil/trace.go:171","msg":"trace[822558558] transaction","detail":"{read_only:false; response_revision:16638; number_of_response:1; }","duration":"810.300245ms","start":"2024-12-09T16:17:15.613954Z","end":"2024-12-09T16:17:16.424255Z","steps":["trace[822558558] 'process raft request'  (duration: 585.001866ms)","trace[822558558] 'compare'  (duration: 224.02338ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-09T16:17:16.424987Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:15.613902Z","time spent":"810.606377ms","remote":"127.0.0.1:48036","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:16631 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-12-09T16:17:16.425374Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"745.743538ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-09T16:17:16.425492Z","caller":"traceutil/trace.go:171","msg":"trace[1518979749] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:16638; }","duration":"745.867685ms","start":"2024-12-09T16:17:15.679608Z","end":"2024-12-09T16:17:16.425475Z","steps":["trace[1518979749] 'agreement among raft nodes before linearized reading'  (duration: 744.653326ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:16.428119Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"645.025055ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1111"}
{"level":"info","ts":"2024-12-09T16:17:16.429004Z","caller":"traceutil/trace.go:171","msg":"trace[1616771155] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:16638; }","duration":"646.015116ms","start":"2024-12-09T16:17:15.782273Z","end":"2024-12-09T16:17:16.428289Z","steps":["trace[1616771155] 'agreement among raft nodes before linearized reading'  (duration: 642.018503ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:16.429190Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:15.782215Z","time spent":"646.947738ms","remote":"127.0.0.1:48014","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1135,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2024-12-09T16:17:16.921200Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"245.578621ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-09T16:17:16.921343Z","caller":"traceutil/trace.go:171","msg":"trace[1657453271] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:16638; }","duration":"245.725439ms","start":"2024-12-09T16:17:16.675586Z","end":"2024-12-09T16:17:16.921311Z","steps":["trace[1657453271] 'range keys from in-memory index tree'  (duration: 245.555637ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:16.921804Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"321.002802ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033774101145475 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:16636 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2024-12-09T16:17:16.921944Z","caller":"traceutil/trace.go:171","msg":"trace[1624611272] linearizableReadLoop","detail":"{readStateIndex:20817; appliedIndex:20816; }","duration":"223.261698ms","start":"2024-12-09T16:17:16.698669Z","end":"2024-12-09T16:17:16.921931Z","steps":["trace[1624611272] 'read index received'  (duration: 40.897µs)","trace[1624611272] 'applied index is now lower than readState.Index'  (duration: 223.216793ms)"],"step_count":2}
{"level":"info","ts":"2024-12-09T16:17:16.922151Z","caller":"traceutil/trace.go:171","msg":"trace[98569330] transaction","detail":"{read_only:false; response_revision:16639; number_of_response:1; }","duration":"462.28573ms","start":"2024-12-09T16:17:16.459851Z","end":"2024-12-09T16:17:16.922137Z","steps":["trace[98569330] 'process raft request'  (duration: 140.78982ms)","trace[98569330] 'compare'  (duration: 320.806208ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-09T16:17:16.922381Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:16.459824Z","time spent":"462.455492ms","remote":"127.0.0.1:48014","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:16636 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-12-09T16:17:16.924632Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"225.904874ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-09T16:17:16.924738Z","caller":"traceutil/trace.go:171","msg":"trace[235224397] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:16639; }","duration":"226.027667ms","start":"2024-12-09T16:17:16.698663Z","end":"2024-12-09T16:17:16.924691Z","steps":["trace[235224397] 'agreement among raft nodes before linearized reading'  (duration: 223.354736ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:18.187627Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"336.889153ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033774101145483 > lease_revoke:<id:70cc93a743ad274e>","response":"size:30"}
{"level":"info","ts":"2024-12-09T16:17:18.187733Z","caller":"traceutil/trace.go:171","msg":"trace[565620905] linearizableReadLoop","detail":"{readStateIndex:20818; appliedIndex:20817; }","duration":"414.878537ms","start":"2024-12-09T16:17:17.772843Z","end":"2024-12-09T16:17:18.187721Z","steps":["trace[565620905] 'read index received'  (duration: 77.93338ms)","trace[565620905] 'applied index is now lower than readState.Index'  (duration: 336.944225ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-09T16:17:18.187864Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"415.014285ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-12-09T16:17:18.187964Z","caller":"traceutil/trace.go:171","msg":"trace[1667696114] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:16639; }","duration":"415.131678ms","start":"2024-12-09T16:17:17.772778Z","end":"2024-12-09T16:17:18.187909Z","steps":["trace[1667696114] 'agreement among raft nodes before linearized reading'  (duration: 414.978948ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-09T16:17:18.188064Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-09T16:17:17.772739Z","time spent":"415.279359ms","remote":"127.0.0.1:48002","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":4,"response size":32,"request content":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true "}


==> etcd [f59804d23cc1] <==
{"level":"warn","ts":"2024-12-14T21:05:35.869646Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"224.50699ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-14T21:05:35.869709Z","caller":"traceutil/trace.go:171","msg":"trace[78291212] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:16808; }","duration":"224.572529ms","start":"2024-12-14T21:05:35.645126Z","end":"2024-12-14T21:05:35.869699Z","steps":["trace[78291212] 'agreement among raft nodes before linearized reading'  (duration: 222.500776ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:05:40.616626Z","caller":"traceutil/trace.go:171","msg":"trace[893798991] transaction","detail":"{read_only:false; response_revision:16812; number_of_response:1; }","duration":"214.17377ms","start":"2024-12-14T21:05:40.402433Z","end":"2024-12-14T21:05:40.616607Z","steps":["trace[893798991] 'process raft request'  (duration: 210.77531ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:05:41.281891Z","caller":"traceutil/trace.go:171","msg":"trace[429866340] transaction","detail":"{read_only:false; response_revision:16813; number_of_response:1; }","duration":"392.51322ms","start":"2024-12-14T21:05:40.889318Z","end":"2024-12-14T21:05:41.281831Z","steps":["trace[429866340] 'process raft request'  (duration: 388.506408ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-14T21:05:41.282086Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-14T21:05:40.889294Z","time spent":"392.691253ms","remote":"127.0.0.1:46522","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:16804 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-12-14T21:05:44.217639Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"149.838099ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033910332719019 > lease_revoke:<id:70cc93c6fbb5ff60>","response":"size:30"}
{"level":"info","ts":"2024-12-14T21:05:44.217875Z","caller":"traceutil/trace.go:171","msg":"trace[579087638] linearizableReadLoop","detail":"{readStateIndex:21023; appliedIndex:21022; }","duration":"164.628551ms","start":"2024-12-14T21:05:44.053214Z","end":"2024-12-14T21:05:44.217842Z","steps":["trace[579087638] 'read index received'  (duration: 11.852895ms)","trace[579087638] 'applied index is now lower than readState.Index'  (duration: 152.770262ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-14T21:05:44.218033Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"164.800889ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-14T21:05:44.218102Z","caller":"traceutil/trace.go:171","msg":"trace[1545527539] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:16814; }","duration":"164.903268ms","start":"2024-12-14T21:05:44.053182Z","end":"2024-12-14T21:05:44.218085Z","steps":["trace[1545527539] 'agreement among raft nodes before linearized reading'  (duration: 164.728773ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:06:06.310670Z","caller":"traceutil/trace.go:171","msg":"trace[2005894463] transaction","detail":"{read_only:false; response_revision:16825; number_of_response:1; }","duration":"104.096991ms","start":"2024-12-14T21:06:06.204566Z","end":"2024-12-14T21:06:06.308663Z","steps":["trace[2005894463] 'process raft request'  (duration: 97.73264ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:06:09.010505Z","caller":"traceutil/trace.go:171","msg":"trace[1045029582] linearizableReadLoop","detail":"{readStateIndex:21039; appliedIndex:21038; }","duration":"102.950832ms","start":"2024-12-14T21:06:08.905418Z","end":"2024-12-14T21:06:09.008368Z","steps":["trace[1045029582] 'read index received'  (duration: 98.24384ms)","trace[1045029582] 'applied index is now lower than readState.Index'  (duration: 4.705053ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-14T21:06:09.015689Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.760651ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1111"}
{"level":"info","ts":"2024-12-14T21:06:09.016982Z","caller":"traceutil/trace.go:171","msg":"trace[1647445802] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:16825; }","duration":"110.995556ms","start":"2024-12-14T21:06:08.905401Z","end":"2024-12-14T21:06:09.016397Z","steps":["trace[1647445802] 'agreement among raft nodes before linearized reading'  (duration: 103.221104ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-14T21:06:09.300569Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"127.567752ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033910332719122 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:16818 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128033910332719118 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2024-12-14T21:06:09.300909Z","caller":"traceutil/trace.go:171","msg":"trace[1779598779] linearizableReadLoop","detail":"{readStateIndex:21040; appliedIndex:21039; }","duration":"196.442196ms","start":"2024-12-14T21:06:09.104389Z","end":"2024-12-14T21:06:09.300832Z","steps":["trace[1779598779] 'read index received'  (duration: 34.58µs)","trace[1779598779] 'applied index is now lower than readState.Index'  (duration: 196.403038ms)"],"step_count":2}
{"level":"info","ts":"2024-12-14T21:06:09.301221Z","caller":"traceutil/trace.go:171","msg":"trace[412477795] transaction","detail":"{read_only:false; response_revision:16826; number_of_response:1; }","duration":"285.955793ms","start":"2024-12-14T21:06:09.014985Z","end":"2024-12-14T21:06:09.300941Z","steps":["trace[412477795] 'process raft request'  (duration: 88.547404ms)","trace[412477795] 'compare'  (duration: 104.290212ms)","trace[412477795] 'store kv pair into bolt db' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:114; } (duration: 21.683412ms)"],"step_count":3}
{"level":"warn","ts":"2024-12-14T21:06:09.305583Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.93607ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-14T21:06:09.306043Z","caller":"traceutil/trace.go:171","msg":"trace[677964612] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:16826; }","duration":"201.27309ms","start":"2024-12-14T21:06:09.104384Z","end":"2024-12-14T21:06:09.305657Z","steps":["trace[677964612] 'agreement among raft nodes before linearized reading'  (duration: 196.629444ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:06:10.701119Z","caller":"traceutil/trace.go:171","msg":"trace[1233650012] transaction","detail":"{read_only:false; response_revision:16827; number_of_response:1; }","duration":"193.702083ms","start":"2024-12-14T21:06:10.506949Z","end":"2024-12-14T21:06:10.700651Z","steps":["trace[1233650012] 'process raft request'  (duration: 189.644606ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-14T21:06:13.991704Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"203.618636ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033910332719143 > lease_revoke:<id:70cc93c6fbb5ffde>","response":"size:30"}
{"level":"info","ts":"2024-12-14T21:06:14.624892Z","caller":"traceutil/trace.go:171","msg":"trace[1757394865] transaction","detail":"{read_only:false; response_revision:16830; number_of_response:1; }","duration":"272.795055ms","start":"2024-12-14T21:06:14.352066Z","end":"2024-12-14T21:06:14.624861Z","steps":["trace[1757394865] 'process raft request'  (duration: 270.884937ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-14T21:06:14.812218Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.55861ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-14T21:06:14.812368Z","caller":"traceutil/trace.go:171","msg":"trace[811203128] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:16830; }","duration":"103.609919ms","start":"2024-12-14T21:06:14.708739Z","end":"2024-12-14T21:06:14.812349Z","steps":["trace[811203128] 'range keys from in-memory index tree'  (duration: 102.23323ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:06:19.600629Z","caller":"traceutil/trace.go:171","msg":"trace[61151388] transaction","detail":"{read_only:false; response_revision:16834; number_of_response:1; }","duration":"151.438929ms","start":"2024-12-14T21:06:19.449046Z","end":"2024-12-14T21:06:19.600485Z","steps":["trace[61151388] 'process raft request'  (duration: 136.720667ms)","trace[61151388] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 12.040631ms)"],"step_count":2}
{"level":"info","ts":"2024-12-14T21:06:23.920278Z","caller":"traceutil/trace.go:171","msg":"trace[1056391817] transaction","detail":"{read_only:false; response_revision:16837; number_of_response:1; }","duration":"224.305359ms","start":"2024-12-14T21:06:23.695910Z","end":"2024-12-14T21:06:23.920215Z","steps":["trace[1056391817] 'process raft request'  (duration: 220.979818ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-14T21:06:24.569458Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"203.592362ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033910332719196 > lease_revoke:<id:70cc93c6fbb6000e>","response":"size:30"}
{"level":"warn","ts":"2024-12-14T21:06:26.285027Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"197.169949ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-14T21:06:26.286927Z","caller":"traceutil/trace.go:171","msg":"trace[1128825200] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:16839; }","duration":"205.101771ms","start":"2024-12-14T21:06:26.081790Z","end":"2024-12-14T21:06:26.286892Z","steps":["trace[1128825200] 'range keys from in-memory index tree'  (duration: 194.62121ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:06:32.942801Z","caller":"traceutil/trace.go:171","msg":"trace[1620317057] transaction","detail":"{read_only:false; response_revision:16843; number_of_response:1; }","duration":"127.409401ms","start":"2024-12-14T21:06:32.815339Z","end":"2024-12-14T21:06:32.942748Z","steps":["trace[1620317057] 'process raft request'  (duration: 126.993635ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:06:33.202488Z","caller":"traceutil/trace.go:171","msg":"trace[1532785185] transaction","detail":"{read_only:false; response_revision:16844; number_of_response:1; }","duration":"373.110658ms","start":"2024-12-14T21:06:32.829301Z","end":"2024-12-14T21:06:33.202412Z","steps":["trace[1532785185] 'process raft request'  (duration: 342.103994ms)","trace[1532785185] 'compare'  (duration: 30.736202ms)"],"step_count":2}
{"level":"info","ts":"2024-12-14T21:06:33.202365Z","caller":"traceutil/trace.go:171","msg":"trace[1465430820] linearizableReadLoop","detail":"{readStateIndex:21062; appliedIndex:21061; }","duration":"291.215117ms","start":"2024-12-14T21:06:32.911066Z","end":"2024-12-14T21:06:33.202281Z","steps":["trace[1465430820] 'read index received'  (duration: 84.506293ms)","trace[1465430820] 'applied index is now lower than readState.Index'  (duration: 206.707806ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-14T21:06:33.205896Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"294.717316ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-14T21:06:33.206126Z","caller":"traceutil/trace.go:171","msg":"trace[905317573] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:0; response_revision:16844; }","duration":"294.931448ms","start":"2024-12-14T21:06:32.911055Z","end":"2024-12-14T21:06:33.205987Z","steps":["trace[905317573] 'agreement among raft nodes before linearized reading'  (duration: 291.506034ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-14T21:06:33.207314Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-14T21:06:32.829262Z","time spent":"373.395826ms","remote":"127.0.0.1:46418","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:16842 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-12-14T21:09:12.940739Z","caller":"traceutil/trace.go:171","msg":"trace[857540070] transaction","detail":"{read_only:false; response_revision:16971; number_of_response:1; }","duration":"114.126485ms","start":"2024-12-14T21:09:12.826187Z","end":"2024-12-14T21:09:12.940313Z","steps":["trace[857540070] 'process raft request'  (duration: 113.992998ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:13:26.916065Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16933}
{"level":"info","ts":"2024-12-14T21:13:26.980364Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":16933,"took":"59.053847ms","hash":2789144461,"current-db-size-bytes":2506752,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":2437120,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2024-12-14T21:13:26.980962Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2789144461,"revision":16933,"compact-revision":16419}
{"level":"info","ts":"2024-12-14T21:18:26.922311Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17211}
{"level":"info","ts":"2024-12-14T21:18:26.938207Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":17211,"took":"14.389883ms","hash":13860878,"current-db-size-bytes":2506752,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1806336,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-12-14T21:18:26.938390Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":13860878,"revision":17211,"compact-revision":16933}
{"level":"warn","ts":"2024-12-14T21:19:54.380051Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"276.184727ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128033910332723336 > lease_revoke:<id:70cc93c6fbb6103b>","response":"size:30"}
{"level":"warn","ts":"2024-12-14T21:19:54.391215Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"242.827805ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-14T21:19:54.389826Z","caller":"traceutil/trace.go:171","msg":"trace[1434567270] linearizableReadLoop","detail":"{readStateIndex:21905; appliedIndex:21904; }","duration":"242.595444ms","start":"2024-12-14T21:19:54.143328Z","end":"2024-12-14T21:19:54.385923Z","steps":["trace[1434567270] 'read index received'  (duration: 46.427µs)","trace[1434567270] 'applied index is now lower than readState.Index'  (duration: 242.547374ms)"],"step_count":2}
{"level":"info","ts":"2024-12-14T21:19:54.393799Z","caller":"traceutil/trace.go:171","msg":"trace[55341346] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:17520; }","duration":"247.95245ms","start":"2024-12-14T21:19:54.143323Z","end":"2024-12-14T21:19:54.391275Z","steps":["trace[55341346] 'agreement among raft nodes before linearized reading'  (duration: 242.734865ms)"],"step_count":1}
{"level":"warn","ts":"2024-12-14T21:19:54.953889Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"221.763809ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-12-14T21:19:54.953687Z","caller":"traceutil/trace.go:171","msg":"trace[765573605] transaction","detail":"{read_only:false; response_revision:17521; number_of_response:1; }","duration":"387.500085ms","start":"2024-12-14T21:19:54.566109Z","end":"2024-12-14T21:19:54.953609Z","steps":["trace[765573605] 'process raft request'  (duration: 385.611643ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:19:54.953992Z","caller":"traceutil/trace.go:171","msg":"trace[1243391020] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:17521; }","duration":"221.875764ms","start":"2024-12-14T21:19:54.732078Z","end":"2024-12-14T21:19:54.953954Z","steps":["trace[1243391020] 'agreement among raft nodes before linearized reading'  (duration: 221.687404ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:19:54.953442Z","caller":"traceutil/trace.go:171","msg":"trace[1209471833] linearizableReadLoop","detail":"{readStateIndex:21906; appliedIndex:21905; }","duration":"221.324576ms","start":"2024-12-14T21:19:54.732099Z","end":"2024-12-14T21:19:54.953424Z","steps":["trace[1209471833] 'read index received'  (duration: 219.671032ms)","trace[1209471833] 'applied index is now lower than readState.Index'  (duration: 1.652843ms)"],"step_count":2}
{"level":"warn","ts":"2024-12-14T21:19:54.963310Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-12-14T21:19:54.566089Z","time spent":"389.650532ms","remote":"127.0.0.1:46418","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:17520 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-12-14T21:21:24.974809Z","caller":"traceutil/trace.go:171","msg":"trace[1151095878] transaction","detail":"{read_only:false; response_revision:17619; number_of_response:1; }","duration":"100.768091ms","start":"2024-12-14T21:21:24.874004Z","end":"2024-12-14T21:21:24.974772Z","steps":["trace[1151095878] 'compare'  (duration: 92.555982ms)"],"step_count":1}
{"level":"info","ts":"2024-12-14T21:23:26.927264Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17450}
{"level":"info","ts":"2024-12-14T21:23:26.936924Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":17450,"took":"8.79012ms","hash":1609741847,"current-db-size-bytes":2506752,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":2035712,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-12-14T21:23:26.937079Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1609741847,"revision":17450,"compact-revision":17211}
{"level":"info","ts":"2024-12-14T21:28:26.939400Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17824}
{"level":"info","ts":"2024-12-14T21:28:26.955831Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":17824,"took":"14.282767ms","hash":1147566038,"current-db-size-bytes":2506752,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":2203648,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2024-12-14T21:28:26.956043Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1147566038,"revision":17824,"compact-revision":17450}
{"level":"info","ts":"2024-12-14T21:33:26.945910Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18098}
{"level":"info","ts":"2024-12-14T21:33:26.963820Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":18098,"took":"15.985582ms","hash":3397003112,"current-db-size-bytes":2506752,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1687552,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-12-14T21:33:26.963937Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3397003112,"revision":18098,"compact-revision":17824}


==> kernel <==
 21:33:29 up  1:09,  0 users,  load average: 0.33, 0.25, 0.40
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [3573cf260c09] <==
I1208 17:14:18.263403       1 shared_informer.go:320] Caches are synced for configmaps
I1208 17:14:18.263403       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1208 17:14:18.264222       1 policy_source.go:224] refreshing policies
I1208 17:14:18.265342       1 shared_informer.go:320] Caches are synced for node_authorizer
I1208 17:14:18.269690       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1208 17:14:18.270259       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1208 17:14:18.272171       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1208 17:14:18.272217       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1208 17:14:18.272406       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1208 17:14:18.272484       1 aggregator.go:171] initial CRD sync complete...
I1208 17:14:18.272630       1 autoregister_controller.go:144] Starting autoregister controller
I1208 17:14:18.272679       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1208 17:14:18.272691       1 cache.go:39] Caches are synced for autoregister controller
I1208 17:14:18.273774       1 cache.go:39] Caches are synced for LocalAvailability controller
I1208 17:14:18.364122       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1208 17:14:18.364152       1 cache.go:39] Caches are synced for RemoteAvailability controller
E1208 17:14:18.369594       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E1208 17:14:18.371852       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I1208 17:14:18.378476       1 controller.go:615] quota admission added evaluator for: namespaces
E1208 17:14:18.468256       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I1208 17:14:18.588364       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1208 17:14:19.126061       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1208 17:14:19.141784       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1208 17:14:19.141832       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1208 17:14:20.548875       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1208 17:14:20.618374       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1208 17:14:20.782877       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1208 17:14:20.822128       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1208 17:14:20.823338       1 controller.go:615] quota admission added evaluator for: endpoints
I1208 17:14:20.830363       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1208 17:14:21.261843       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1208 17:14:22.173695       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1208 17:14:22.207240       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1208 17:14:22.266689       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1208 17:14:28.442750       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1208 17:14:28.465642       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I1208 17:19:01.564788       1 alloc.go:330] "allocated clusterIPs" service="default/frontend-service" clusterIPs={"IPv4":"10.106.163.166"}
E1208 20:06:15.982296       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1208 20:06:15.896314       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1208 21:07:46.101011       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1208 21:07:46.101054       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1208 22:21:08.515242       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1208 22:21:08.515246       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1209 00:11:01.778249       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1209 00:11:01.778253       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1209 12:49:28.782280       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1209 12:49:28.782279       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1209 13:51:18.544063       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1209 13:51:18.544081       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1209 16:03:00.504822       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1209 16:03:00.504818       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1209 16:13:53.112552       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1209 16:13:53.113194       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 1.29698121s, panicked: false, err: context deadline exceeded, panic-reason: <nil>" logger="UnhandledError"
E1209 16:13:53.113177       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1209 16:13:53.508289       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1209 16:13:53.508390       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1209 16:13:53.510479       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1209 16:13:53.510933       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1209 16:13:53.603563       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="808.551618ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E1209 16:13:53.603568       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="811.412764ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null


==> kube-apiserver [ea721f236a36] <==
I1214 21:03:31.413365       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1214 21:03:31.413543       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1214 21:03:31.414554       1 controller.go:142] Starting OpenAPI controller
I1214 21:03:31.414718       1 controller.go:90] Starting OpenAPI V3 controller
I1214 21:03:31.414997       1 naming_controller.go:294] Starting NamingConditionController
I1214 21:03:31.415239       1 establishing_controller.go:81] Starting EstablishingController
I1214 21:03:31.415337       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1214 21:03:31.415485       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1214 21:03:31.415521       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1214 21:03:31.415668       1 aggregator.go:169] waiting for initial CRD sync...
I1214 21:03:31.415702       1 crd_finalizer.go:269] Starting CRDFinalizer
I1214 21:03:31.416667       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1214 21:03:31.417158       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1214 21:03:31.417204       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1214 21:03:31.417262       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1214 21:03:31.417709       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1214 21:03:31.417753       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1214 21:03:31.417840       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1214 21:03:31.417872       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1214 21:03:31.418752       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1214 21:03:31.419547       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1214 21:03:31.421607       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1214 21:03:31.417173       1 controller.go:119] Starting legacy_token_tracking_controller
I1214 21:03:31.421919       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1214 21:03:31.411701       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1214 21:03:31.422461       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1214 21:03:31.422546       1 controller.go:78] Starting OpenAPI AggregationController
I1214 21:03:31.670172       1 shared_informer.go:320] Caches are synced for configmaps
I1214 21:03:31.770091       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1214 21:03:31.770596       1 aggregator.go:171] initial CRD sync complete...
I1214 21:03:31.770898       1 autoregister_controller.go:144] Starting autoregister controller
I1214 21:03:31.771047       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1214 21:03:31.869525       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1214 21:03:31.870248       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1214 21:03:31.870271       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1214 21:03:31.870334       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1214 21:03:31.872498       1 cache.go:39] Caches are synced for LocalAvailability controller
I1214 21:03:31.873646       1 cache.go:39] Caches are synced for autoregister controller
I1214 21:03:31.876543       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1214 21:03:31.877684       1 shared_informer.go:320] Caches are synced for node_authorizer
I1214 21:03:31.878514       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1214 21:03:31.879692       1 policy_source.go:224] refreshing policies
I1214 21:03:32.070462       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1214 21:03:32.092355       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E1214 21:03:32.171012       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 51838639-549f-4b4f-bb1d-1509088ad653, UID in object meta: "
E1214 21:03:32.171742       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1214 21:03:32.672002       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1214 21:03:38.878888       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1214 21:03:38.896433       1 controller.go:615] quota admission added evaluator for: endpoints
E1214 21:04:56.097315       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1214 21:04:56.693768       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 4.210779902s, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E1214 21:04:56.906173       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1214 21:04:57.092220       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1214 21:04:57.924640       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="8.99908518s" method="PUT" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
I1214 21:11:27.364462       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1214 21:11:27.425803       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1214 21:13:19.669167       1 alloc.go:330] "allocated clusterIPs" service="default/cybersecurity-chatbot-service" clusterIPs={"IPv4":"10.110.216.232"}
I1214 21:22:18.746742       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1214 21:22:18.761739       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1214 21:24:45.874508       1 alloc.go:330] "allocated clusterIPs" service="default/cybersecurity-chatbot-service" clusterIPs={"IPv4":"10.99.225.84"}


==> kube-controller-manager [3f9470148785] <==
I1214 21:03:39.177726       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="599.284459ms"
I1214 21:03:39.180611       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="111.124µs"
I1214 21:03:42.143342       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="80.606µs"
I1214 21:03:43.164221       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="8.262743ms"
I1214 21:03:43.164546       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="110.362µs"
I1214 21:03:44.215083       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="11.292006ms"
I1214 21:03:44.215551       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="75.342µs"
I1214 21:03:46.315388       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="27.322785ms"
I1214 21:03:46.315573       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="55.451µs"
I1214 21:04:47.500027       1 request.go:700] Waited for 4.492127546s due to client-side throttling, not priority and fairness, request: GET:https://192.168.49.2:8443/api
I1214 21:08:35.236909       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1214 21:11:27.473235       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="40.568436ms"
I1214 21:11:27.491017       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="17.650292ms"
I1214 21:11:27.504408       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="13.304857ms"
I1214 21:11:27.505139       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="619.61µs"
I1214 21:11:27.513986       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="2.569712ms"
I1214 21:11:27.514108       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="51.098µs"
I1214 21:11:27.585164       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="2.010331ms"
I1214 21:11:27.595380       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="156.167µs"
I1214 21:11:33.519190       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="5.872003ms"
I1214 21:11:33.519331       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="42.902µs"
I1214 21:11:34.540961       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="5.698753ms"
I1214 21:11:34.541103       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="66.787µs"
I1214 21:11:48.786485       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1214 21:16:55.669467       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1214 21:20:25.068780       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="323.988µs"
I1214 21:21:25.179037       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="447.350836ms"
I1214 21:21:25.291380       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="112.232495ms"
I1214 21:21:25.302927       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="11.455744ms"
I1214 21:21:25.393876       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="90.855088ms"
I1214 21:21:25.393996       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="55.836µs"
I1214 21:21:26.379073       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="75.724µs"
I1214 21:21:26.407119       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="75.263µs"
I1214 21:21:27.928894       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="57.82µs"
I1214 21:21:27.944412       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="69.642µs"
I1214 21:21:27.950004       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="63.691µs"
I1214 21:21:27.975600       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="99.579µs"
I1214 21:21:27.986846       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="41.388µs"
I1214 21:21:27.989437       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="75.003µs"
I1214 21:21:35.022707       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="6.21634ms"
I1214 21:21:35.022846       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="44.554µs"
I1214 21:21:37.047298       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="7.342085ms"
I1214 21:21:37.047435       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="41.519µs"
I1214 21:22:01.941119       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1214 21:22:15.980257       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="84.532654ms"
I1214 21:22:16.073963       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="93.596283ms"
I1214 21:22:16.099658       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="25.593163ms"
I1214 21:22:16.188234       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="88.396304ms"
I1214 21:22:16.188533       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="145.587µs"
I1214 21:22:16.491785       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-frontend-5784f46789" duration="17.684µs"
I1214 21:24:39.550343       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="29.923593ms"
I1214 21:24:39.580875       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="30.430205ms"
I1214 21:24:39.594278       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="13.310196ms"
I1214 21:24:39.594430       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="67.028µs"
I1214 21:24:40.610893       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="5.827218ms"
I1214 21:24:40.611209       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="64.292µs"
I1214 21:24:40.619466       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="4.312438ms"
I1214 21:24:40.619583       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ouassim012-cybersecurity-57658fd7dc" duration="53.572µs"
I1214 21:27:07.193193       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1214 21:32:12.337295       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [8ce9cc7c8fb8] <==
I1208 19:02:10.599106       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 19:07:18.637508       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 19:12:26.870191       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 19:17:33.441150       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 19:22:39.952121       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 19:27:46.044956       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 19:32:51.504774       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 20:06:16.090320       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1208 20:06:16.091593       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1208 20:08:38.935441       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 21:07:14.991743       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 21:07:46.109584       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1208 21:07:46.111113       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1208 21:38:58.819668       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 22:21:08.528279       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1208 22:21:08.528905       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1208 22:25:19.357526       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 22:30:25.269829       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 22:35:30.799185       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 22:40:36.885418       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 22:45:42.555683       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 22:50:48.816341       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 22:55:53.779800       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 23:01:00.135638       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 23:06:06.908724       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 23:11:14.049030       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 23:16:20.749017       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 23:21:27.364735       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 23:26:33.925709       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 23:31:39.945006       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1208 23:54:39.409276       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:01:41.487349       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:06:48.970442       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:11:01.787112       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1209 00:11:01.788331       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1209 00:11:56.781768       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:17:02.341414       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:22:08.513549       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:27:14.058683       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:32:21.074486       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:37:27.076978       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:42:34.179550       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:47:39.971448       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 00:52:45.549095       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 12:49:28.893929       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1209 12:49:28.976424       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1209 12:52:44.839150       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 12:57:51.766747       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 13:02:59.358254       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 13:08:06.269642       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 13:13:13.182087       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 13:18:19.224070       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 13:51:18.558541       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1209 13:51:18.583351       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1209 13:52:12.597329       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 14:11:31.878403       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
E1209 16:03:00.528067       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1209 16:03:00.526998       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1209 16:08:56.684130       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1209 16:14:11.437995       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [075f30b03d14] <==
E1208 17:14:32.043288       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1208 17:14:32.043797       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1208 17:14:32.064348       1 server_linux.go:66] "Using iptables proxy"
I1208 17:14:32.479256       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1208 17:14:32.479431       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1208 17:14:32.542222       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1208 17:14:32.542287       1 server_linux.go:169] "Using iptables Proxier"
I1208 17:14:32.545631       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1208 17:14:32.546470       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1208 17:14:32.547030       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1208 17:14:32.547181       1 server.go:483] "Version info" version="v1.31.0"
I1208 17:14:32.547203       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1208 17:14:32.550669       1 config.go:197] "Starting service config controller"
I1208 17:14:32.550734       1 shared_informer.go:313] Waiting for caches to sync for service config
I1208 17:14:32.550770       1 config.go:104] "Starting endpoint slice config controller"
I1208 17:14:32.550779       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1208 17:14:32.550852       1 config.go:326] "Starting node config controller"
I1208 17:14:32.550897       1 shared_informer.go:313] Waiting for caches to sync for node config
I1208 17:14:32.651585       1 shared_informer.go:320] Caches are synced for node config
I1208 17:14:32.651670       1 shared_informer.go:320] Caches are synced for service config
I1208 17:14:32.651702       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [c8e43d7d0e4a] <==
E1214 21:03:40.324616       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1214 21:03:40.333953       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1214 21:03:40.372765       1 server_linux.go:66] "Using iptables proxy"
I1214 21:03:41.277503       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1214 21:03:41.277643       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1214 21:03:41.344682       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1214 21:03:41.344858       1 server_linux.go:169] "Using iptables Proxier"
I1214 21:03:41.348795       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1214 21:03:41.379853       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1214 21:03:41.401058       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1214 21:03:41.401929       1 server.go:483] "Version info" version="v1.31.0"
I1214 21:03:41.401975       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1214 21:03:41.410915       1 config.go:197] "Starting service config controller"
I1214 21:03:41.412183       1 config.go:104] "Starting endpoint slice config controller"
I1214 21:03:41.413390       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1214 21:03:41.413714       1 shared_informer.go:313] Waiting for caches to sync for service config
I1214 21:03:41.415677       1 config.go:326] "Starting node config controller"
I1214 21:03:41.422825       1 shared_informer.go:313] Waiting for caches to sync for node config
I1214 21:03:41.516815       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1214 21:03:41.516872       1 shared_informer.go:320] Caches are synced for service config
I1214 21:03:41.523018       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [ef566f746431] <==
I1214 21:03:25.188844       1 serving.go:386] Generated self-signed cert in-memory
W1214 21:03:31.786766       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1214 21:03:31.787047       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1214 21:03:31.787078       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1214 21:03:31.794135       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1214 21:03:32.282644       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1214 21:03:32.283075       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1214 21:03:32.304609       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1214 21:03:32.369756       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1214 21:03:32.369701       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1214 21:03:32.370226       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1214 21:03:32.483053       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [fa113f593492] <==
W1208 17:14:18.501073       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1208 17:14:18.502231       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1208 17:14:18.502548       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1208 17:14:18.504660       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1208 17:14:18.505082       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1208 17:14:18.512278       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1208 17:14:18.512456       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1208 17:14:18.512857       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1208 17:14:18.512891       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1208 17:14:18.513058       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1208 17:14:18.513085       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1208 17:14:18.563102       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1208 17:14:18.563204       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1208 17:14:18.563169       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1208 17:14:18.563492       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1208 17:14:18.563539       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1208 17:14:18.563647       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1208 17:14:18.563681       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1208 17:14:18.563599       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1208 17:14:18.563750       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1208 17:14:18.563776       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E1208 17:14:18.563780       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1208 17:14:18.563774       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E1208 17:14:18.563869       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1208 17:14:18.504674       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1208 17:14:18.564163       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1208 17:14:18.563724       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1208 17:14:18.564250       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1208 17:14:18.563217       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.378151       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1208 17:14:19.378248       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.470578       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1208 17:14:19.470684       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.616454       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1208 17:14:19.616541       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.739900       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1208 17:14:19.739967       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.783567       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1208 17:14:19.783637       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.815478       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1208 17:14:19.815552       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.819379       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1208 17:14:19.819475       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1208 17:14:19.878814       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1208 17:14:19.878884       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.886786       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1208 17:14:19.886879       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.910661       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1208 17:14:19.910746       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.911485       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1208 17:14:19.911546       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1208 17:14:19.976669       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1208 17:14:19.976734       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1208 17:14:20.028388       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1208 17:14:20.028498       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1208 17:14:20.116607       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1208 17:14:20.116680       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1208 17:14:20.160483       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1208 17:14:20.160546       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
I1208 17:14:21.500674       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Dec 14 21:21:27 minikube kubelet[1682]: I1214 21:21:27.934814    1682 scope.go:117] "RemoveContainer" containerID="5ef0a37ad669e194d813c43cf2a56580bf4f6a170d99fe2d5acb1a04ebffde37"
Dec 14 21:21:28 minikube kubelet[1682]: I1214 21:21:28.704773    1682 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="d76f256e-132f-4387-9a6b-b92b567659a1" path="/var/lib/kubelet/pods/d76f256e-132f-4387-9a6b-b92b567659a1/volumes"
Dec 14 21:21:28 minikube kubelet[1682]: I1214 21:21:28.707886    1682 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="f939a7bc-265c-4e3b-ab5a-ea30a8c59a7f" path="/var/lib/kubelet/pods/f939a7bc-265c-4e3b-ab5a-ea30a8c59a7f/volumes"
Dec 14 21:21:35 minikube kubelet[1682]: I1214 21:21:35.016836    1682 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/angular-frontend-5784f46789-4zlhj" podStartSLOduration=2.596126789 podStartE2EDuration="10.016447019s" podCreationTimestamp="2024-12-14 21:21:25 +0000 UTC" firstStartedPulling="2024-12-14 21:21:26.956545577 +0000 UTC m=+1093.466216614" lastFinishedPulling="2024-12-14 21:21:34.376783945 +0000 UTC m=+1100.886536844" observedRunningTime="2024-12-14 21:21:35.01628424 +0000 UTC m=+1101.526037150" watchObservedRunningTime="2024-12-14 21:21:35.016447019 +0000 UTC m=+1101.526199908"
Dec 14 21:21:37 minikube kubelet[1682]: I1214 21:21:37.039744    1682 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/angular-frontend-5784f46789-jgmdr" podStartSLOduration=3.765715432 podStartE2EDuration="13.039720795s" podCreationTimestamp="2024-12-14 21:21:24 +0000 UTC" firstStartedPulling="2024-12-14 21:21:26.957465402 +0000 UTC m=+1093.467136439" lastFinishedPulling="2024-12-14 21:21:36.231388913 +0000 UTC m=+1102.741141802" observedRunningTime="2024-12-14 21:21:37.039330763 +0000 UTC m=+1103.549083662" watchObservedRunningTime="2024-12-14 21:21:37.039720795 +0000 UTC m=+1103.549473684"
Dec 14 21:22:15 minikube kubelet[1682]: E1214 21:22:15.982592    1682 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="d76f256e-132f-4387-9a6b-b92b567659a1" containerName="angular-frontend"
Dec 14 21:22:15 minikube kubelet[1682]: E1214 21:22:15.982754    1682 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f939a7bc-265c-4e3b-ab5a-ea30a8c59a7f" containerName="angular-frontend"
Dec 14 21:22:15 minikube kubelet[1682]: I1214 21:22:15.982866    1682 memory_manager.go:354] "RemoveStaleState removing state" podUID="f939a7bc-265c-4e3b-ab5a-ea30a8c59a7f" containerName="angular-frontend"
Dec 14 21:22:15 minikube kubelet[1682]: I1214 21:22:15.982882    1682 memory_manager.go:354] "RemoveStaleState removing state" podUID="d76f256e-132f-4387-9a6b-b92b567659a1" containerName="angular-frontend"
Dec 14 21:22:16 minikube kubelet[1682]: E1214 21:22:16.076322    1682 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="d76f256e-132f-4387-9a6b-b92b567659a1" containerName="angular-frontend"
Dec 14 21:22:16 minikube kubelet[1682]: E1214 21:22:16.076389    1682 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f939a7bc-265c-4e3b-ab5a-ea30a8c59a7f" containerName="angular-frontend"
Dec 14 21:22:16 minikube kubelet[1682]: I1214 21:22:16.076439    1682 memory_manager.go:354] "RemoveStaleState removing state" podUID="d76f256e-132f-4387-9a6b-b92b567659a1" containerName="angular-frontend"
Dec 14 21:22:16 minikube kubelet[1682]: I1214 21:22:16.076456    1682 memory_manager.go:354] "RemoveStaleState removing state" podUID="f939a7bc-265c-4e3b-ab5a-ea30a8c59a7f" containerName="angular-frontend"
Dec 14 21:22:16 minikube kubelet[1682]: I1214 21:22:16.175547    1682 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sftgh\" (UniqueName: \"kubernetes.io/projected/f651ad21-95b0-41b3-8fc1-9c4c6c829509-kube-api-access-sftgh\") pod \"angular-frontend-5784f46789-99pgf\" (UID: \"f651ad21-95b0-41b3-8fc1-9c4c6c829509\") " pod="default/angular-frontend-5784f46789-99pgf"
Dec 14 21:22:16 minikube kubelet[1682]: I1214 21:22:16.276664    1682 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sh9pn\" (UniqueName: \"kubernetes.io/projected/a8e935b0-f305-42ec-8788-b9561821bd9f-kube-api-access-sh9pn\") pod \"angular-frontend-5784f46789-jljq7\" (UID: \"a8e935b0-f305-42ec-8788-b9561821bd9f\") " pod="default/angular-frontend-5784f46789-jljq7"
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.286028    1682 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"kube-api-access-k7ck6\" (UniqueName: \"kubernetes.io/projected/66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8-kube-api-access-k7ck6\") pod \"66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8\" (UID: \"66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8\") "
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.293926    1682 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8-kube-api-access-k7ck6" (OuterVolumeSpecName: "kube-api-access-k7ck6") pod "66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8" (UID: "66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8"). InnerVolumeSpecName "kube-api-access-k7ck6". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.387792    1682 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-k7ck6\" (UniqueName: \"kubernetes.io/projected/66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8-kube-api-access-k7ck6\") on node \"minikube\" DevicePath \"\""
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.417148    1682 scope.go:117] "RemoveContainer" containerID="89fc744cefa78791040c64cfd78b19e5c489702005ed1fc50f67123efc46e831"
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.488719    1682 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"kube-api-access-cvkhb\" (UniqueName: \"kubernetes.io/projected/5f46af83-2bab-49af-9f64-47303a71952d-kube-api-access-cvkhb\") pod \"5f46af83-2bab-49af-9f64-47303a71952d\" (UID: \"5f46af83-2bab-49af-9f64-47303a71952d\") "
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.494937    1682 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/5f46af83-2bab-49af-9f64-47303a71952d-kube-api-access-cvkhb" (OuterVolumeSpecName: "kube-api-access-cvkhb") pod "5f46af83-2bab-49af-9f64-47303a71952d" (UID: "5f46af83-2bab-49af-9f64-47303a71952d"). InnerVolumeSpecName "kube-api-access-cvkhb". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.499539    1682 scope.go:117] "RemoveContainer" containerID="89fc744cefa78791040c64cfd78b19e5c489702005ed1fc50f67123efc46e831"
Dec 14 21:22:17 minikube kubelet[1682]: E1214 21:22:17.506653    1682 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 89fc744cefa78791040c64cfd78b19e5c489702005ed1fc50f67123efc46e831" containerID="89fc744cefa78791040c64cfd78b19e5c489702005ed1fc50f67123efc46e831"
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.506898    1682 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"89fc744cefa78791040c64cfd78b19e5c489702005ed1fc50f67123efc46e831"} err="failed to get container status \"89fc744cefa78791040c64cfd78b19e5c489702005ed1fc50f67123efc46e831\": rpc error: code = Unknown desc = Error response from daemon: No such container: 89fc744cefa78791040c64cfd78b19e5c489702005ed1fc50f67123efc46e831"
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.506932    1682 scope.go:117] "RemoveContainer" containerID="862a259b6b7062e3598b1fa4af05ef2b4adb4905c492aa01d3c9c136bc8ef713"
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.589332    1682 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-cvkhb\" (UniqueName: \"kubernetes.io/projected/5f46af83-2bab-49af-9f64-47303a71952d-kube-api-access-cvkhb\") on node \"minikube\" DevicePath \"\""
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.590077    1682 scope.go:117] "RemoveContainer" containerID="862a259b6b7062e3598b1fa4af05ef2b4adb4905c492aa01d3c9c136bc8ef713"
Dec 14 21:22:17 minikube kubelet[1682]: E1214 21:22:17.591165    1682 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 862a259b6b7062e3598b1fa4af05ef2b4adb4905c492aa01d3c9c136bc8ef713" containerID="862a259b6b7062e3598b1fa4af05ef2b4adb4905c492aa01d3c9c136bc8ef713"
Dec 14 21:22:17 minikube kubelet[1682]: I1214 21:22:17.591220    1682 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"862a259b6b7062e3598b1fa4af05ef2b4adb4905c492aa01d3c9c136bc8ef713"} err="failed to get container status \"862a259b6b7062e3598b1fa4af05ef2b4adb4905c492aa01d3c9c136bc8ef713\": rpc error: code = Unknown desc = Error response from daemon: No such container: 862a259b6b7062e3598b1fa4af05ef2b4adb4905c492aa01d3c9c136bc8ef713"
Dec 14 21:22:18 minikube kubelet[1682]: I1214 21:22:18.704296    1682 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="5f46af83-2bab-49af-9f64-47303a71952d" path="/var/lib/kubelet/pods/5f46af83-2bab-49af-9f64-47303a71952d/volumes"
Dec 14 21:22:18 minikube kubelet[1682]: I1214 21:22:18.705204    1682 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8" path="/var/lib/kubelet/pods/66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8/volumes"
Dec 14 21:22:20 minikube kubelet[1682]: I1214 21:22:20.554269    1682 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/angular-frontend-5784f46789-99pgf" podStartSLOduration=3.50513766 podStartE2EDuration="5.55417609s" podCreationTimestamp="2024-12-14 21:22:15 +0000 UTC" firstStartedPulling="2024-12-14 21:22:17.476788724 +0000 UTC m=+1143.986584771" lastFinishedPulling="2024-12-14 21:22:19.525827144 +0000 UTC m=+1146.035623201" observedRunningTime="2024-12-14 21:22:20.553913262 +0000 UTC m=+1147.063709309" watchObservedRunningTime="2024-12-14 21:22:20.55417609 +0000 UTC m=+1147.063972157"
Dec 14 21:22:21 minikube kubelet[1682]: I1214 21:22:21.216013    1682 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"kube-api-access-sftgh\" (UniqueName: \"kubernetes.io/projected/f651ad21-95b0-41b3-8fc1-9c4c6c829509-kube-api-access-sftgh\") pod \"f651ad21-95b0-41b3-8fc1-9c4c6c829509\" (UID: \"f651ad21-95b0-41b3-8fc1-9c4c6c829509\") "
Dec 14 21:22:21 minikube kubelet[1682]: I1214 21:22:21.218310    1682 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f651ad21-95b0-41b3-8fc1-9c4c6c829509-kube-api-access-sftgh" (OuterVolumeSpecName: "kube-api-access-sftgh") pod "f651ad21-95b0-41b3-8fc1-9c4c6c829509" (UID: "f651ad21-95b0-41b3-8fc1-9c4c6c829509"). InnerVolumeSpecName "kube-api-access-sftgh". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 14 21:22:21 minikube kubelet[1682]: I1214 21:22:21.316706    1682 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-sftgh\" (UniqueName: \"kubernetes.io/projected/f651ad21-95b0-41b3-8fc1-9c4c6c829509-kube-api-access-sftgh\") on node \"minikube\" DevicePath \"\""
Dec 14 21:22:21 minikube kubelet[1682]: I1214 21:22:21.555431    1682 scope.go:117] "RemoveContainer" containerID="6f3f722368a51d4d7e3c5c181efad26ad051eaa7f82134b1daaf9b93ac5b31be"
Dec 14 21:22:21 minikube kubelet[1682]: I1214 21:22:21.575314    1682 scope.go:117] "RemoveContainer" containerID="6f3f722368a51d4d7e3c5c181efad26ad051eaa7f82134b1daaf9b93ac5b31be"
Dec 14 21:22:21 minikube kubelet[1682]: E1214 21:22:21.576664    1682 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 6f3f722368a51d4d7e3c5c181efad26ad051eaa7f82134b1daaf9b93ac5b31be" containerID="6f3f722368a51d4d7e3c5c181efad26ad051eaa7f82134b1daaf9b93ac5b31be"
Dec 14 21:22:21 minikube kubelet[1682]: I1214 21:22:21.576735    1682 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"6f3f722368a51d4d7e3c5c181efad26ad051eaa7f82134b1daaf9b93ac5b31be"} err="failed to get container status \"6f3f722368a51d4d7e3c5c181efad26ad051eaa7f82134b1daaf9b93ac5b31be\": rpc error: code = Unknown desc = Error response from daemon: No such container: 6f3f722368a51d4d7e3c5c181efad26ad051eaa7f82134b1daaf9b93ac5b31be"
Dec 14 21:22:22 minikube kubelet[1682]: I1214 21:22:22.582733    1682 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/angular-frontend-5784f46789-jljq7" podStartSLOduration=3.531955299 podStartE2EDuration="7.582714367s" podCreationTimestamp="2024-12-14 21:22:15 +0000 UTC" firstStartedPulling="2024-12-14 21:22:17.476979731 +0000 UTC m=+1143.986775778" lastFinishedPulling="2024-12-14 21:22:21.527738799 +0000 UTC m=+1148.037534846" observedRunningTime="2024-12-14 21:22:22.582563832 +0000 UTC m=+1149.092359900" watchObservedRunningTime="2024-12-14 21:22:22.582714367 +0000 UTC m=+1149.092510414"
Dec 14 21:22:22 minikube kubelet[1682]: I1214 21:22:22.711724    1682 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="f651ad21-95b0-41b3-8fc1-9c4c6c829509" path="/var/lib/kubelet/pods/f651ad21-95b0-41b3-8fc1-9c4c6c829509/volumes"
Dec 14 21:22:23 minikube kubelet[1682]: I1214 21:22:23.231367    1682 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"kube-api-access-sh9pn\" (UniqueName: \"kubernetes.io/projected/a8e935b0-f305-42ec-8788-b9561821bd9f-kube-api-access-sh9pn\") pod \"a8e935b0-f305-42ec-8788-b9561821bd9f\" (UID: \"a8e935b0-f305-42ec-8788-b9561821bd9f\") "
Dec 14 21:22:23 minikube kubelet[1682]: I1214 21:22:23.234003    1682 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/a8e935b0-f305-42ec-8788-b9561821bd9f-kube-api-access-sh9pn" (OuterVolumeSpecName: "kube-api-access-sh9pn") pod "a8e935b0-f305-42ec-8788-b9561821bd9f" (UID: "a8e935b0-f305-42ec-8788-b9561821bd9f"). InnerVolumeSpecName "kube-api-access-sh9pn". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 14 21:22:23 minikube kubelet[1682]: I1214 21:22:23.332467    1682 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-sh9pn\" (UniqueName: \"kubernetes.io/projected/a8e935b0-f305-42ec-8788-b9561821bd9f-kube-api-access-sh9pn\") on node \"minikube\" DevicePath \"\""
Dec 14 21:22:23 minikube kubelet[1682]: I1214 21:22:23.584846    1682 scope.go:117] "RemoveContainer" containerID="17e16268b6e3bde2cafad25eeb2a338a493e1ef40898d791765ce0287480cde9"
Dec 14 21:22:23 minikube kubelet[1682]: I1214 21:22:23.599525    1682 scope.go:117] "RemoveContainer" containerID="17e16268b6e3bde2cafad25eeb2a338a493e1ef40898d791765ce0287480cde9"
Dec 14 21:22:23 minikube kubelet[1682]: E1214 21:22:23.600464    1682 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 17e16268b6e3bde2cafad25eeb2a338a493e1ef40898d791765ce0287480cde9" containerID="17e16268b6e3bde2cafad25eeb2a338a493e1ef40898d791765ce0287480cde9"
Dec 14 21:22:23 minikube kubelet[1682]: I1214 21:22:23.600508    1682 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"17e16268b6e3bde2cafad25eeb2a338a493e1ef40898d791765ce0287480cde9"} err="failed to get container status \"17e16268b6e3bde2cafad25eeb2a338a493e1ef40898d791765ce0287480cde9\": rpc error: code = Unknown desc = Error response from daemon: No such container: 17e16268b6e3bde2cafad25eeb2a338a493e1ef40898d791765ce0287480cde9"
Dec 14 21:22:24 minikube kubelet[1682]: I1214 21:22:24.703857    1682 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="a8e935b0-f305-42ec-8788-b9561821bd9f" path="/var/lib/kubelet/pods/a8e935b0-f305-42ec-8788-b9561821bd9f/volumes"
Dec 14 21:24:39 minikube kubelet[1682]: E1214 21:24:39.546864    1682 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f651ad21-95b0-41b3-8fc1-9c4c6c829509" containerName="angular-frontend"
Dec 14 21:24:39 minikube kubelet[1682]: E1214 21:24:39.546951    1682 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="5f46af83-2bab-49af-9f64-47303a71952d" containerName="angular-frontend"
Dec 14 21:24:39 minikube kubelet[1682]: E1214 21:24:39.546970    1682 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="a8e935b0-f305-42ec-8788-b9561821bd9f" containerName="angular-frontend"
Dec 14 21:24:39 minikube kubelet[1682]: E1214 21:24:39.546987    1682 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8" containerName="angular-frontend"
Dec 14 21:24:39 minikube kubelet[1682]: I1214 21:24:39.547109    1682 memory_manager.go:354] "RemoveStaleState removing state" podUID="66d81ac3-bcc4-4b8f-8523-e1bd22e40fe8" containerName="angular-frontend"
Dec 14 21:24:39 minikube kubelet[1682]: I1214 21:24:39.547127    1682 memory_manager.go:354] "RemoveStaleState removing state" podUID="5f46af83-2bab-49af-9f64-47303a71952d" containerName="angular-frontend"
Dec 14 21:24:39 minikube kubelet[1682]: I1214 21:24:39.547140    1682 memory_manager.go:354] "RemoveStaleState removing state" podUID="f651ad21-95b0-41b3-8fc1-9c4c6c829509" containerName="angular-frontend"
Dec 14 21:24:39 minikube kubelet[1682]: I1214 21:24:39.547156    1682 memory_manager.go:354] "RemoveStaleState removing state" podUID="a8e935b0-f305-42ec-8788-b9561821bd9f" containerName="angular-frontend"
Dec 14 21:24:39 minikube kubelet[1682]: I1214 21:24:39.657412    1682 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rqmth\" (UniqueName: \"kubernetes.io/projected/cd64dcc8-ba81-4fa2-86bf-e94d6290a444-kube-api-access-rqmth\") pod \"ouassim012-cybersecurity-57658fd7dc-zcgsw\" (UID: \"cd64dcc8-ba81-4fa2-86bf-e94d6290a444\") " pod="default/ouassim012-cybersecurity-57658fd7dc-zcgsw"
Dec 14 21:24:39 minikube kubelet[1682]: I1214 21:24:39.657563    1682 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4h9f9\" (UniqueName: \"kubernetes.io/projected/ca39c5b2-1e75-4ec3-a4e4-7d0ea192aed9-kube-api-access-4h9f9\") pod \"ouassim012-cybersecurity-57658fd7dc-4k7bh\" (UID: \"ca39c5b2-1e75-4ec3-a4e4-7d0ea192aed9\") " pod="default/ouassim012-cybersecurity-57658fd7dc-4k7bh"
Dec 14 21:24:40 minikube kubelet[1682]: I1214 21:24:40.605702    1682 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/ouassim012-cybersecurity-57658fd7dc-4k7bh" podStartSLOduration=1.605627036 podStartE2EDuration="1.605627036s" podCreationTimestamp="2024-12-14 21:24:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-12-14 21:24:40.605284865 +0000 UTC m=+1287.115387980" watchObservedRunningTime="2024-12-14 21:24:40.605627036 +0000 UTC m=+1287.115730161"


==> storage-provisioner [709f0e4aa89e] <==
I1214 21:05:41.399527       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1214 21:05:42.680300       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1214 21:05:42.684793       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1214 21:06:01.187789       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"393ae729-c0d9-4e1a-baba-fbaaff0e4a81", APIVersion:"v1", ResourceVersion:"16819", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_4c663017-94fc-4cfc-9375-fbd142117f00 became leader
I1214 21:06:01.190852       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1214 21:06:01.192092       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_4c663017-94fc-4cfc-9375-fbd142117f00!
I1214 21:06:01.600745       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_4c663017-94fc-4cfc-9375-fbd142117f00!


==> storage-provisioner [9bfe22566d3e] <==
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00025db80)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00025db80, 0x18b3d60, 0xc000227380, 0x1, 0xc0001e42a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00025db80, 0x3b9aca00, 0x0, 0x1, 0xc0001e42a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc00025db80, 0x3b9aca00, 0xc0001e42a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 140 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0xc000112850, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000112840)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc000110420, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc000141680, 0x18e5530, 0xc0003c8140, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00025dba0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00025dba0, 0x18b3d60, 0xc0002273b0, 0x1, 0xc0001e42a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00025dba0, 0x3b9aca00, 0x0, 0x1, 0xc0001e42a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc00025dba0, 0x3b9aca00, 0xc0001e42a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 141 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0xc000112a10, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000112a00)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0001105a0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc000141680, 0x18e5530, 0xc0003c8140, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00025dbe0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00025dbe0, 0x18b3d60, 0xc0000a4ea0, 0x1, 0xc0001e42a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00025dbe0, 0x3b9aca00, 0x0, 0x1, 0xc0001e42a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc00025dbe0, 0x3b9aca00, 0xc0001e42a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

